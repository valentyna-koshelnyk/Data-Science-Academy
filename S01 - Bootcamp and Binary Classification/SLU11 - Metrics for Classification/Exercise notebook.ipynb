{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-46d843154dcd7d0f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Validation Metrics for Classification - Exercise notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8d860e3a9483163",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-95c595096027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_confusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/classifier_prediction_scores.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'probas'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/batch3-workspace/S01 - Bootcamp and Binary Classification/SLU11 - Metrics for Classification/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "import numpy as np\n",
    "from utils import show_confusion_matrix, hash_answer\n",
    "from matplotlib import pyplot as plt \n",
    "data = pd.read_csv('data/classifier_prediction_scores.csv').rename(columns={'scores': 'probas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a34bd8a41530e62",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this notebook the following is tested:\n",
    "\n",
    "- Understanding the problem with accuracy\n",
    "- Understanding FP, TP, FN, TN\n",
    "- Understanding precision, recall\n",
    "- Understanding the ROC Curve \n",
    "- Understanding AUROC\n",
    "- Which model is better for a particular circumstance \n",
    "- Using these metrics in day to day \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f89a55ca33111154",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Useful information\n",
    "\n",
    "In this exercise we have the following dataset: \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>probas</th>      <th>target</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0.288467</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0.255047</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>0.201017</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>0.729307</td>      <td>1</td>    </tr>    <tr>      <th>4</th>      <td>0.148288</td>      <td>0</td>    </tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d34c4723b41a782",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1 - Understanding the problem with Accuracy \n",
    "\n",
    "Do the following steps without using scikit: \n",
    "\n",
    "1. Create a column called \"predicted at 0.5 threshold\", which will be:\n",
    "> 0 if probas is smaller than 0.5  \n",
    "> 1 otherwise (larger or equal to 0.5)   \n",
    "2. Calculate the accuracy of the predictions (feel free to use a support column called \"correct prediction\")\n",
    "3. Calculate what the accuracy would have been if you had simply predicted 0 every time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72ba6d5cbf2be8e0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def threshold_probas(proba, threshold=.5): \n",
    "    if proba >= threshold:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0 \n",
    "\n",
    "def accuracy_analysis(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # df[\"predicted at 0.5 threshold\"] = ...\n",
    "    # hint: the function you just calcualted \n",
    "    # YOUR CODE HERE\n",
    "    df[\"predicted at 0.5 threshold\"] = data.predicted_proba.map(threshold_probas)\n",
    "    \n",
    "    # df[\"correct prediction\"] = ... (optional but useful)\n",
    "    # YOUR CODE HERE\n",
    "    df[\"correct prediction\"] = data['prediction'] == data['disease']\n",
    "    # accuracy_of_predictions = ... \n",
    "    # YOUR CODE HERE\n",
    "    number_of_correct_predictions = df['correct prediction'].sum()\n",
    "    total_number_of_predictions = data.shape[0]\n",
    "\n",
    "    accuracy = number_of_correct_predictions / total_number_of_predictions \n",
    "    \n",
    "    \n",
    "    # accuracy_predicting_always_zero = ... (approximately 3 to 5 lines)\n",
    "    # YOUR CODE HERE\n",
    "    realistic_data['always zero'] = 0 \n",
    "    realistic_data['correct answer'] = realistic_data['always zero'] == realistic_data['disease']\n",
    "    number_of_correct_predictions = realistic_data['correct answer'].sum()\n",
    "    total_number_of_predictions = realistic_data.shape[0]\n",
    "\n",
    "    accuracy = number_of_correct_predictions / total_number_of_predictions\n",
    "    \n",
    "    return accuracy_of_predictions, accuracy_predicting_always_zero\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff217b96e0d0697f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'predicted_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-05a82f986a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predictions accuracy:               %0.3f \\nAccuracy of predicting always zero: %0.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5690be15c232>\u001b[0m in \u001b[0;36maccuracy_analysis\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# hint: the function you just calcualted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted at 0.5 threshold\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_proba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold_probas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# df[\"correct prediction\"] = ... (optional but useful)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/slu10/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'predicted_proba'"
     ]
    }
   ],
   "source": [
    "results = accuracy_analysis(data)\n",
    "print('Predictions accuracy:               %0.3f \\nAccuracy of predicting always zero: %0.3f' % (results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76914a3b73cc4208",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "```\n",
    "Predictions accuracy:               0.830 \n",
    "Accuracy of predicting always zero: 0.834\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-26325384c86158fa",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(results[0], float)\n",
    "assert isinstance(results[1], float)\n",
    "assert math.isclose(results[1] - results[0], 0.00319, abs_tol=.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d025a0824fb08ef3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Understanding main metrics at a particular threshold \n",
    "\n",
    "For the following confusion matrix, without using scikit, calculate (manually) the number of \n",
    "- False Positives    (FP)\n",
    "- True Positives     (TP)\n",
    "- False Negatives    (FN)\n",
    "- True Negatives     (TN)\n",
    "\n",
    "along with the \n",
    "- True Positive Rate  (TPR)\n",
    "- False Positive Rate (FPR)\n",
    "- Recall\n",
    "- Precision\n",
    "\n",
    "Note: you can hardcode the number in this one :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-408a2b5a616ccfaa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bfc98777980f60ed",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def my_confusion_matrix_analysis():\n",
    "    # true_positives = ...\n",
    "    # YOUR CODE HERE\n",
    "    true_positives = \n",
    "    \n",
    "\n",
    "    # true_negatives = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # false_positives = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # false_negatives = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # false_positives_rate = ...  (a.k.a False Alarm Rate!)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # true_positive_rate = ... \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # recall = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return true_positives, true_negatives, false_positives, false_negatives, \\\n",
    "           false_positive_rate, true_positive_rate, precision, recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0312a1da01fce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "TP, TN, FP, FN, FPR, TPR, precision, recall = my_confusion_matrix_analysis()\n",
    "\n",
    "print('True Positives:       %0.0f' % TP)\n",
    "print('True Negatives:       %0.0f' % TN)\n",
    "print('False Positives:      %0.0f' % FP)\n",
    "print('False Negatives:      %0.0f' % FN)\n",
    "print('False Positive rate:  %0.2f' % FPR)\n",
    "print('True Positive rate:   %0.2f' % TPR)\n",
    "print('precision:            %0.2f' % precision)\n",
    "print('recall:               %0.2f' % recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1a3d33376c70afb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "    True Positives:       <not telling you!>\n",
    "    True Negatives:       <not telling you!>\n",
    "    False Positives:      <not telling you!>\n",
    "    False Negatives:      <not telling you!>\n",
    "    False Positive rate:  0.30\n",
    "    True Positive rate:   0.57\n",
    "    precision:            0.28\n",
    "    recall:               0.57\n",
    "    \n",
    "_Note: your `True Positive Rate` and `Recall` should be the same because... ehm... (check their formula!)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b80f2ff83f63bac8",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hash_answer(round(TP,3)) == '3038bfb575bee6a0e61945eff8784835bb2c720634e42734678c083994b7f018'\n",
    "assert hash_answer(round(TN,3)) == 'f24f1a64b591544a871284bdde332d3c5d2cb109d21c03122c57d768e7c535b1'\n",
    "assert hash_answer(round(FP,3)) == '20e9c64c05a54d199610fb7e38135361324b5ed5dcf39c23afe9b48926c07376'\n",
    "assert hash_answer(round(FN,3)) == 'cd70bea023f752a0564abb6ed08d42c1440f2e33e29914e55e0be1595e24f45a'\n",
    "assert math.isclose(FPR, 0.298, abs_tol=0.01)\n",
    "assert math.isclose(TPR, 0.572, abs_tol=0.01)\n",
    "assert math.isclose(precision, 0.277, abs_tol=0.01)\n",
    "assert math.isclose(recall, 0.572, abs_tol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e542b0c2502309e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: model selection \n",
    "\n",
    "Consider the following two confusion matrixes: \n",
    "\n",
    "<img src=\"data/conf_mats.png\" alt=\"drawing\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b50fa7cfe6ed2f83",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# which option (A or B) is best, if this is a model for screening for cancer? \n",
    "# In this situation: \n",
    "# - 1 is cancer\n",
    "# - 0 is healtthy\n",
    "# - predicting 1 sends people to a cancer screening\n",
    "# - predicting 0 sends people home \n",
    "\n",
    "# best_option_for_cancer_screening = ... (\"A\" or \"B\")\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# what option (A or B) is better if you are evaluating the performance of a judge over his career? \n",
    "# In this situation: \n",
    "# - 1 is guilty\n",
    "# - 0 is innocent \n",
    "# - predicting 1 sends people to jail\n",
    "# - predicting 0 sends people home free\n",
    "\n",
    "# best_option_for_a_judge = ... (\"A\" or \"B\")\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f6e108311857c10",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hash_answer(best_option_for_cancer_screening.lower()) == '3e23e8160039594a33894f6564e1b1348bbd7a0088d42c4acb73eeaed59c009d'\n",
    "assert hash_answer(best_option_for_a_judge.lower()) == 'ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0fbe84422a1518aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5: Understanding the ROC Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0e4693c249ada78",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# for your conveninence, we've already written most of this.\n",
    "# the majority of this exercise is understanding what is going on, and completing the last functions :)\n",
    "\n",
    "def threshold_probas(proba, threshold): \n",
    "    # returns the thresholded prediction for a single observation \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     proba (float): a scores or probability, between 0.0 and 1.0\n",
    "    #     threshold (float): a number between 0.0 and 1.0    \n",
    "    # -----------------------------------------------------\n",
    "    # example: threshold_probas(.9, .5) = 1\n",
    "    if proba >= threshold:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0 \n",
    "    \n",
    "def get_predictions(probas, threshold):\n",
    "    # returns the thresholded predictions \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     probas (pd.Series): a series of floats (scores or probabilities) between 0.0 and 1.0\n",
    "    #     threshold (float): a number between 0.0 and 1.0    \n",
    "    # -----------------------------------------------------\n",
    "    # example: get_predictions([.1, .2, .5], .4) = [0, 0 1]\n",
    "    # -----------------------------------------------------\n",
    "    return probas.map(lambda x: threshold_probas(x, threshold))\n",
    "\n",
    "def calculate_number_of_true_positives(predictions, target):\n",
    "    # calculates the number of true positives \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_true_positives([[0, 1, 0]], [0, 1, 1]) = 1 \n",
    "    # -----------------------------------------------------\n",
    "    positive_predictions_indexes = (predictions == 1)\n",
    "    true_positives = predictions.loc[positive_predictions_indexes] == target[positive_predictions_indexes]\n",
    "    return true_positives.sum()\n",
    "    \n",
    "def calculate_number_of_false_positives(predictions, target): \n",
    "    # calculates the number of false positives \n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_false_positives([[0, 1, 0]], [0, 1, 1]) = 0\n",
    "    # -----------------------------------------------------\n",
    "    positive_predictions_indexes = (predictions == 1)\n",
    "    false_positives = predictions.loc[positive_predictions_indexes] != target[positive_predictions_indexes]\n",
    "    return false_positives.sum()\n",
    "    \n",
    "def calculate_number_of_true_negatives(predictions, target): \n",
    "    # calculates the number of true_negatives\n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_true_negatives([[0, 1, 0]], [0, 1, 1]) = 1 \n",
    "    # -----------------------------------------------------\n",
    "    negative_predictions_indexes = (predictions == 0)\n",
    "    true_negatives = predictions.loc[negative_predictions_indexes] == target[negative_predictions_indexes]\n",
    "    return true_negatives.sum()\n",
    "\n",
    "def calculate_number_of_false_negatives(predictions, target): \n",
    "    # calculates the number of false_negatives\n",
    "    # -----------------------------------------------------\n",
    "    # inputs: \n",
    "    #     predictions (pd.Series): a series of ints, either 0s or 1s, with predictions\n",
    "    #     target (pd.Series): a series of ints, either 0s or 1s, with observed outcomes \n",
    "    # -----------------------------------------------------\n",
    "    # example: calculate_number_of_false_negatives([[0, 1, 0]], [0, 1, 1]) = 1 \n",
    "    # -----------------------------------------------------\n",
    "    negative_predictions_indexes = (predictions == 0)\n",
    "    false_negatives = predictions.loc[negative_predictions_indexes] != target[negative_predictions_indexes]\n",
    "    return false_negatives.sum()\n",
    "\n",
    "def calculate_true_positive_rate(predictions, target):\n",
    "    # calculate the true positive rate\n",
    "    # hint: we've defined most things above, so 2 of the lines are just function calls \n",
    "    # true_positive_rate = ... (~ 3 lines)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return true_positive_rate\n",
    "\n",
    "def calculate_false_positive_rate(predictions, target):\n",
    "    # calculate the false positive rate \n",
    "    # hint: we've defined most things above, so 2 of the lines are just function calls \n",
    "    # false_positive_rate = ... (~ 3 lines)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return true_positive_rate\n",
    "\n",
    "def hand_made_roc_curve(probas, target, list_of_thresholds):\n",
    "    \n",
    "    TPRs = {}\n",
    "    FPRs = {}\n",
    "    \n",
    "    # for each threshold, get the predictions, and calculate the TPR and FPR\n",
    "    # hint: you already defined everything above, so each line should be just a function call   \n",
    "    for threshold in list_of_thresholds:\n",
    "        # predictions = \n",
    "        # TPRs[threshold] = ...\n",
    "        # FPRs[threshold] = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return {'True Positive Rate': TPRs, 'False Positive Rate': FPRs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5b547305337b81d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# generating a space of thresholds (we'll be more \"dense near zero\")\n",
    "list_of_thresholds = np.linspace(0, 10) ** 2 / 100\n",
    "\n",
    "# using the function you have just created \n",
    "hand_made_roc = hand_made_roc_curve(data['probas'], data['target'], list_of_thresholds)\n",
    "\n",
    "# just making a dataframe to pretty things up\n",
    "hand_made_roc_df = pd.DataFrame({'False Positive Rate': hand_made_roc['False Positive Rate'], \n",
    "                        'True Positive Rate': hand_made_roc['True Positive Rate']})\n",
    "\n",
    "# naming the index\n",
    "hand_made_roc_df.index.name = 'Thresholds'\n",
    "\n",
    "# plotting the ROC curve \n",
    "hand_made_roc_df.set_index('False Positive Rate').plot();\n",
    "plt.show()\n",
    "\n",
    "# displaying the ROC curve \n",
    "display(hand_made_roc_df.iloc[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25a6f9756a2a30ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>fpr</th>      <th>tpr</th>    </tr>    <tr>      <th>Thresholds</th>      <th></th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th>0.041649</th>      <td>0.964491</td>      <td>0.995192</td>    </tr>    <tr>      <th>0.050396</th>      <td>0.924184</td>      <td>0.995192</td>    </tr>    <tr>      <th>0.059975</th>      <td>0.879079</td>      <td>0.990385</td>    </tr>    <tr>      <th>0.070387</th>      <td>0.833013</td>      <td>0.975962</td>    </tr>    <tr>      <th>0.081633</th>      <td>0.786948</td>      <td>0.956731</td>    </tr>    <tr>      <th>0.093711</th>      <td>0.741843</td>      <td>0.942308</td>    </tr>    <tr>      <th>0.106622</th>      <td>0.688100</td>      <td>0.899038</td>    </tr>    <tr>      <th>0.120367</th>      <td>0.619002</td>      <td>0.826923</td>    </tr>    <tr>      <th>0.134944</th>      <td>0.544146</td>      <td>0.783654</td>    </tr>    <tr>      <th>0.150354</th>      <td>0.467370</td>      <td>0.745192</td>    </tr>  </tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5d25008fa21ed0ab",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "hand_made_roc = hand_made_roc_curve(data['probas'], data['target'], list_of_thresholds)\n",
    "hand_made_roc_df = pd.DataFrame({'False Positive Rate': hand_made_roc['False Positive Rate'], \n",
    "                        'True Positive Rate': hand_made_roc['True Positive Rate']})\n",
    "\n",
    "assert math.isclose(hand_made_roc_df['False Positive Rate'].iloc[15], 0.741, abs_tol=.001)\n",
    "assert math.isclose(hand_made_roc_df['True Positive Rate'].sum(), 23.85, abs_tol=.01)\n",
    "assert hand_made_roc_df.shape == (50, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56dc6b11c62e5e0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 6: Understanding AUROC (Area under the ROC Curve)\n",
    "\n",
    "Great! You (hopefully) made a ROC curve! \n",
    "\n",
    "We also established that an important metric was the AUROC, which was, quite simply, the area under the curve. \n",
    "\n",
    "Instead of using super-sophisticated methods for calculating areas, we've just printed some rectangles on top of (your!) ROC curve, and printed the heights. \n",
    "\n",
    "All rectangles have a side of 0.1.\n",
    "\n",
    "<img src=\"data/roc_with_squares.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cf1dc7dc8eaea8b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# what is the area under the ROC curve? (approximately, using the rectangles)\n",
    "\n",
    "# approximate_area_under_the_ROC_curve = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d42f7040126fd74b",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hash_answer(round(approximate_area_under_the_ROC_curve, 3)\n",
    "                  ) == '973b372a514f91db8219fef585cdf81b09196afd7948f5fb1e29c2016dd995a0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3422c5927ff8fbf9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 7: The joy of scikit \n",
    "Ok, now you (finally) get to use scikit :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-525a214e5b698b83",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = data.iloc[0:100].copy()\n",
    "df['predictions'] = get_predictions(df['probas'], threshold=.3)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86d92e523ba4389c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def prediction_analyisis_with_scikit(predictions, target):\n",
    "    # importing the correct functions, get the accuracy, precision and recall\n",
    "    \n",
    "    # from ? import ... \n",
    "    # accuracy = ...\n",
    "    # precision = ...\n",
    "    # recall = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "def confusion_matrix_with_scikit(predictions, target):\n",
    "    # importing the correct functions, return scikit's confusion matrix\n",
    "    # from ? import ...\n",
    "    # conf_mat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return conf_mat\n",
    "\n",
    "def get_roc_curve_data_with_scikit(probas, target):\n",
    "    # from ? import ... \n",
    "    # ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "\n",
    "def roc_auc_with_scikit(probas, target):\n",
    "    # from ? import ... \n",
    "    # ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return roc_auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9752ffe16d8a1b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(prediction_analyisis_with_scikit(df['predictions'], df['target']))\n",
    "print(confusion_matrix_with_scikit(df['predictions'], df['target']))\n",
    "\n",
    "fpr_test, tpr_test, thresholds_test = get_roc_curve_data_with_scikit(df['probas'], df['target'])\n",
    "print(fpr_test[20:30])\n",
    "print(tpr_test[20:30])\n",
    "print(thresholds_test[30:45])\n",
    "print(roc_auc_with_scikit(df['probas'], df['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b38ffdbde384e011",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output: \n",
    "\n",
    "    (0.8, 0.5, 0.2)\n",
    "    [[76  4]\n",
    "     [16  4]]\n",
    "    [0.3125 0.4375 0.4375 0.5875 0.5875 0.6125 0.6125 0.65   0.65   0.725 ]\n",
    "    [0.7  0.7  0.75 0.75 0.8  0.8  0.85 0.85 0.9  0.9 ]\n",
    "    [0.08661403 0.03556623 0.02952571 0.01969409]\n",
    "    0.7024999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-056c6c812d48d02d",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "small = df.iloc[0:50]\n",
    "\n",
    "assert prediction_analyisis_with_scikit(small['predictions'], small['target']) == (0.76, 0.5, 0.25)\n",
    "assert math.isclose(roc_auc_with_scikit(small['probas'], small['target']), 0.725, abs_tol=0.01)\n",
    "assert np.allclose(confusion_matrix_with_scikit(small['predictions'], small['target']), [[35, 3], [9, 3]])\n",
    "fpr_test, tpr_test, thresholds_test = get_roc_curve_data_with_scikit(small['probas'], small['target'])\n",
    "assert np.allclose(fpr_test[7:10], [0.15789474, 0.15789474, 0.21052632], atol=.01)\n",
    "assert np.allclose(tpr_test[7:10], [0.33333333, 0.41666667, 0.41666667], atol=.01)\n",
    "assert np.allclose(thresholds_test[7:10], [0.28082532, 0.25504668, 0.24489729], atol=.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
