{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./resources/storeid_201908251010.csv', ',')\n",
    "df2 = pd.read_csv('./resources/targets_201908251011.csv', ',')\n",
    "\n",
    "df3 = pd.merge(df1, df2, on='id', how='outer')\n",
    "\n",
    "missing_api_values = df3[df3['storeid'] == 'API']\n",
    "missing_website_values = df3[df3['storeid'] == 'WEBSITE']\n",
    "# df3 = df3.set_index('id')\n",
    "# missing_api_values = missing_api_values.head(100)\n",
    "# missing_api_values\n",
    "# missing_api_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_api_values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = []\n",
    "current_line = 1\n",
    "cap_size = 1000\n",
    "warn_size = 20\n",
    "\n",
    "for index, row in missing_api_values.iterrows():\n",
    "    \n",
    "    if current_line % warn_size == 0:\n",
    "        print(current_line)\n",
    "        \n",
    "        if current_line % cap_size == 0:\n",
    "            create_csv(my_list,'missing_api_values_'+ str(current_line) +'.csv')\n",
    "            my_list = []\n",
    "        \n",
    "    current_line += 1\n",
    "    \n",
    "    base_url = 'https://y29rdnycjd.execute-api.eu-west-1.amazonaws.com/dev/missingdata/'\n",
    "    search_for = row.id\n",
    "\n",
    "    my_missing_opts = requests.get(base_url + search_for)\n",
    "    \n",
    "    if my_missing_opts.status_code != 404:\n",
    "        my_missing_opts = my_missing_opts.json()\n",
    "        \n",
    "        my_missing_opts['returned'] = row.returned\n",
    "        my_list.append(my_missing_opts)\n",
    "\n",
    "    else:\n",
    "        print('not found')\n",
    "        \n",
    "# missing_api_values.head()\n",
    "\n",
    "# my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_keys = ['id','orderportalid','orderdate_gmt','designer','style','shipper','shiptypeid','userid','isvip','country','region','ddprate','countrycode','hasusedwishlist','isreseller','hasitemsonbag','tierafterorder','tierbeforeorder','isusingmultipledevices','userfraudstatus','promocode','freereturn','issale','productid','brand','ddpsubcategory','storeid','countryoforigin','size','category_1stlevel','platform']\n",
    "my_dict_list = []\n",
    "my_dict = {}\n",
    "\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        link = 'https://scrap-me.herokuapp.com/items?page=' + str(page)\n",
    "        r = requests.get(link)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data)\n",
    "        rows = soup.find(\"tbody\").find_all(\"tr\")\n",
    "        page += 1\n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            for index, feature in enumerate(cells):\n",
    "                my_dict[list_of_keys[index]] = feature.get_text()\n",
    "            my_dict_list.append(my_dict)\n",
    "            my_dict = {}\n",
    "            \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        break # exit `while` loop\n",
    "\n",
    "# print(my_dict_list)\n",
    "create_csv(my_dict_list)\n",
    "#         print()\n",
    "#     print('id', 'orderportalid', 'orderdate_gmt', 'designer', 'style', 'shipper', 'shiptypeid', 'userid', 'isvip', 'country', 'region', 'ddprate', 'countrycode', 'hasusedwishlist', 'isreseller', 'hasitemsonbag', 'tierafterorder', 'tierbeforeorder', 'isusingmultipledevices', 'userfraudstatus \tpromocode \tfreereturn \tissale \tproductid \tbrand \tddpsubcategory \tstoreid \tcountryoforigin \tsize \tcategory_1stlevel \tplatform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(data, name):\n",
    "    \"\"\" returns (file_basename, server_path, file_size) \"\"\"\n",
    "    file_basename = name\n",
    "    server_path = './resources/'\n",
    "    w_file = open(server_path + file_basename, 'w', newline='')\n",
    "    keys = data[0].keys()\n",
    "\n",
    "    with w_file as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data)\n",
    "\n",
    "    w_file.close()\n",
    "\n",
    "    w_file = open(server_path + file_basename, 'r')\n",
    "    file_size = len(w_file.read())\n",
    "    return file_basename, server_path, file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_1 = pd.read_csv('./resources/missing_api_values_1000.csv', ',')\n",
    "\n",
    "for x in range(2,11):\n",
    "    new_pd = pd.read_csv('./resources/missing_api_values_'+ str(x) +'000.csv', ',')\n",
    "    md_1 = md_1.append(new_pd)\n",
    "    \n",
    "# md_1.drop(columns=['returned'])\n",
    "md_1.to_csv(r'resources\\full_api.csv', index=False)\n",
    "# md_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api = pd.read_csv('./resources/full_api.csv', ',')\n",
    "# api = api.drop(columns=['returned'])\n",
    "web = pd.read_csv('./resources/missing_website_values.csv', ',')\n",
    "# api = api.append(web)\n",
    "# api.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./resources/storeid_201908251010.csv', ',')\n",
    "df2 = pd.read_csv('./resources/targets_201908251011.csv', ',')\n",
    "\n",
    "df3 = pd.merge(df1, df2, on='id', how='outer')\n",
    "missing_website_values = df3[df3['storeid'] == 'API']\n",
    "missing_website_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(df3, df2, on='id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_website_values = df3[df3['storeid'] == 'WEBSITE']\n",
    "missing_website_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop(columns=['storeid'])\n",
    "final_web = pd.merge(df3, web, on='id', how='inner')\n",
    "final_web.to_csv(r'resources\\fullweb.csv', index=False)\n",
    "final_web.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_f = pd.read_csv('./resources/full_api.csv', ',')\n",
    "web_f = pd.read_csv('./resources/fullweb.csv', ',')\n",
    "web_f = web_f.drop(columns=['id'])\n",
    "web_f = web_f.append(api_f)\n",
    "web_f.to_csv(r'resources\\full_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_web[final_web['id'] == 'a0d86abe2f9a44abecbabbef612c90c4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_web_api = pd.merge(final_web, api, on='id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_web = final_web.to_csv(r'resources\\fullweb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
