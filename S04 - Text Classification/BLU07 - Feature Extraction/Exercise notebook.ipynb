{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12bea12324c032d8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/valentynakoshelnyk/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f07b8631beb0508c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q1. Country names\n",
    "\n",
    "For the first question you will be making use of regex. In particular you have a list of countries and you'll have to answer some very specific questions about that list.\n",
    "\n",
    "Start by loading the defining the path to this list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe61fa6cbbdef77d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/countries.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52faf4f77a990570",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The first thing you will build is a wrapper that will apply a regex pattern into a given file, and return a list of results found matching that pattern. Implement it in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a88528290dd35f7d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_all_in_file(pattern, path):\n",
    "    \"\"\"\n",
    "    Function that returns all matches of a certain pattern in a certain text.\n",
    "    \n",
    "    Args:\n",
    "    pattern - regex pattern\n",
    "    path - path to the file\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "    lineList = [line.rstrip('\\n') for line in open(path)]\n",
    "    return [i for i in lineList if re.findall(pattern, i)]\n",
    "\n",
    "\n",
    "   \n",
    "        \n",
    "        \n",
    "    \n",
    "   \n",
    "                \n",
    "        \n",
    "       \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef7991b98e752180",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Make sure this function is working with the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a056266aa5ada57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert find_all_in_file(pattern=\"^P.+?$\", path=path)[8] == \"Portugal\"\n",
    "assert find_all_in_file(pattern=\"^.+?a$\", path=path)[18] == \"Croatia\"\n",
    "assert len(find_all_in_file(pattern=\"^.+?ca$\", path=path)) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df341d1f1838c29e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.a)\n",
    "\n",
    "Now that you prepared your wrapper, let's move on to the actual expressions. The first thing we are looking for is for countries with loooong names. In particular we want you to find all the countries with more than 15 letters. Use the wrapper you defined above and assign its return to a variable `ret`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cddd8e2e48afb31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret_long = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_long = find_all_in_file(pattern = '^.{15,}$', path = path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5a358ed83c473214",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries with more than 15 or more letters:  16\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries with more than 15 or more letters: \", len(ret_long))\n",
    "assert len(ret_long) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6727f8213243afc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.b)\n",
    "\n",
    "Now, find out how many countries:\n",
    "* Start with a vowel\n",
    "* Start with a consonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-447eb675481e47fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret_vowel = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_vowel = find_all_in_file(pattern = \"^[aieouAIEOU].*\", path = path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-535072609d84d577",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries that start with vowels:  36\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries that start with vowels: \" , len(ret_vowel))\n",
    "assert len(ret_vowel) == 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66af8912995d91a2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret_consonant = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_consonant = find_all_in_file(pattern =\"^[^aeyiuoAIEOU].*\", path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1c17b943b63b52ae",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries that start with consonants:  160\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries that start with consonants: \" , len(ret_consonant))\n",
    "assert len(ret_consonant) == 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e3cc736b933d71e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.c)\n",
    "\n",
    "Next, find how many countries are composed by only one word and end in `ia`. You'll want to have a list with countries such as `Croatia`, `Serbia`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bf14a2df3dd3e4e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret_ia = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-19b61469674be299",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ret_ia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-93512e9c5e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of variants of countries ending in \\\"ia\\\": \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_ia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m\"Serbia\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret_ia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m\"Croatia\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret_ia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_ia\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ret_ia' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Number of variants of countries ending in \\\"ia\\\": \" , len(ret_ia))\n",
    "assert \"Serbia\" in ret_ia\n",
    "assert \"Croatia\" in ret_ia\n",
    "assert len(ret_ia) == 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6bb573c7736db8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q1.d)\n",
    "\n",
    "Finally, find the countries which have at least four consecutive consonants, without taking into account the first letter (Hint: you can assume the first letter is capitalized). So, it should match things like `Abcdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e92132a5b63400f3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# ret_bcdf = find_all_in_file(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ret_bcdf = find_all_in_file(pattern = '(?:(?![aeiou])[a-z]){4,}', path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5150c254b592778e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of countries matched:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of countries matched: \" , len(ret_bcdf))\n",
    "assert len(ret_bcdf) == 3\n",
    "assert hashlib.sha256(' '.join(ret_bcdf).encode()).hexdigest() == '7da1a15074b9245ae3b88fb92fc5c484243003a084d03280bf11d9346d768869'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0409818769bbf3d0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2. A Study in Scarlet\n",
    "\n",
    "For this following questions we will be looking at Sir Arthur Conan Doyle's [\"A Study in Scarlet\"](https://en.wikipedia.org/wiki/A_Study_in_Scarlet) (which you might have seen adapted to tv in [\"A Study in Pink\"](https://en.wikipedia.org/wiki/A_Study_in_Pink)). We will be performing common preprocessing operations on this text, as it is a common task in Natural Language Processing. Start by downloading the data and loading it into a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abce95c72c255d16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/sherlock.txt\"\n",
    "data =  [line.strip('\\n') for line in open(path, 'r', encoding='utf8') if len(line)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-addc0c904c359402",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.a)\n",
    "\n",
    "First tokenize the data. Implement the function to receive an NLTK-style tokenizer and return the token list for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b7930a61806e32",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_tokenizer(data, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with the tokens of given text. I.e\n",
    "    for an input ['Abc def', 'Ghi jkl mn'] it returns [['Abc', 'def'], ['Ghi', 'jkl', 'mn']]\n",
    "    \n",
    "    Args:\n",
    "    data - list with the data\n",
    "    tokenizer - nltk tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    return [tokenizer.tokenize(i) for i in data]\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f829c5a222c54690",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "data_tok = apply_tokenizer(data=data, tokenizer=tokenizer)\n",
    "\n",
    "assert len(data_tok) == 3770\n",
    "assert len([w for s in data_tok for w in s]) == 51648\n",
    "assert data_tok[8] == ['I','could','join','it',',','the','second','Afghan','war','had','broken','out','.','On','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b76d100b971a777f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.b)\n",
    "\n",
    "The second step you will implement is lowercasing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee47fb5a45fbd622",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_lowercase(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with all the tokens lowecased.\n",
    "    \n",
    "    Args:\n",
    "    data - list with tokenized data\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return [[w.lower() for w in line] for line in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7979e12840663ea2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_lc = apply_lowercase(data=apply_tokenizer(data=data, tokenizer=tokenizer))\n",
    "\n",
    "assert len(data_lc) == 3770\n",
    "assert len([w for s in data_lc for w in s]) == 51648\n",
    "assert data_lc[8] == ['i','could','join','it',',','the','second','afghan','war','had','broken','out','.','on','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8044c14c20583cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.c)\n",
    "\n",
    "Now implement a function that filters the stopwords.\n",
    "\n",
    "NOTE: Stopwords adapted from [here](https://gist.github.com/sebleier/554280). (Notice what we added some specific things, like ?\" and .\" to the stopwords. This was shown to be a limitation of the nltk tokenizer so it will be removed that way, instead of the more conventional way. This goes to show that there are more powerful tokenizers that you should use in the case you have to perform tokenization in the future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ecb29c8fe117f1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_stopwords(data, stopwords_fp):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no stopwords.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stopwords_fp - path to the stopwords file\n",
    "    \"\"\"\n",
    "    #data_filt = [token for token in apply_lowercase(apply_tokenizer(data, tokenizer)) if not token in stopwords]\n",
    "    # Create the list of stopwords from the file\n",
    "    # stopwords = ...\n",
    "    # YOUR CODE HERE\n",
    "    # Filter the stopwords from the text\n",
    "    # data_filt = ...\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    stopwords = [line.strip() for line in open(stopwords_fp)]\n",
    "\n",
    "\n",
    "\n",
    "    data=apply_lowercase(data)\n",
    "    data_filt= list(map(lambda x: [i for i in x if not i.lower() in stopwords], data))\n",
    "  \n",
    "  \n",
    "    return data_filt\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f70da0255ea6e291",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords_fp = \"data/english_stopwords.txt\"\n",
    "data_filt_sw = apply_filter_stopwords(data=apply_lowercase(apply_tokenizer(data, tokenizer)), \n",
    "                                      stopwords_fp=stopwords_fp)\n",
    "assert len(data_filt_sw) == 3770\n",
    "assert len([w for s in data_filt_sw for w in s]) == 27733\n",
    "assert data_filt_sw[8] == ['could', 'join', ',', 'second', 'afghan', 'war', 'broken', '.', 'landing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d0cb317596faa2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.d)\n",
    "\n",
    "After filtering stopwords, we want to remove punctuation from the text as well. Make use of `string.punctuation` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0cd3cf5cc97a8f2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_punkt(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no punctuation.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    import string \n",
    "    exclude = set(string.punctuation)\n",
    "    data =  list(map(lambda x: [i for i in x if not i in exclude], data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-650a677a3a01d1bf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_filt_punkt = apply_filter_punkt(data=apply_tokenizer(data, tokenizer))\n",
    "\n",
    "assert len(data_filt_punkt) == 3770\n",
    "assert len([w for s in data_filt_punkt for w in s]) == 46362\n",
    "assert data_filt_punkt[8] == ['I','could','join','it','the','second','Afghan','war','had','broken','out','On','landing','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38d66dd89731e114",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.e)\n",
    "\n",
    "The last preprocessing step you are going to implement is stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a831ba989f3e50e6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_stemmer(data, stemmer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with stemmed data.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stemmer - instance of stemmer to use\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    data =  [[stemmer.stem(word) for word in sentence] for sentence in data]\n",
    "    \n",
    "    return list(data)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3596a6510ebbda3d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "data_stems = apply_stemmer(data=apply_lowercase(apply_tokenizer(data, tokenizer)),\n",
    "                           stemmer=stemmer)\n",
    "\n",
    "assert len(data_stems) == 3770\n",
    "assert len([w for s in data_stems for w in s]) == 51648\n",
    "assert data_stems[8][-2] == 'land'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bfe5aed6ebdbb26",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q2.f)\n",
    "\n",
    "Finally, join everything in a function, that applies the steps in the following order, in :\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Filtering stopwords\n",
    "* Filtering punctuation\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea5b2305431c20dd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list, lower=True, remove_punct=True, stopwords=[]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, sentences):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        # sentences_tokens = ...\n",
    "        # YOUR CODE HERE\n",
    "        sentences_tokens = [tokenizer.tokenize(i) for i in sentences]\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = [[w.lower() for w in line] for line in sentences]\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            exclude = set(string.punctuation)\n",
    "            sentences_tokens = list(map(lambda x: [i for i in x if not i in exclude], sentences))\n",
    "\n",
    "        if self.stopwords:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            stopwords = [line.strip() for line in open(stopwords_fp)]\n",
    "\n",
    "\n",
    "\n",
    "            data=apply_lowercase(sentences)\n",
    "            data_filt= list(map(lambda x: [i for i in x if not i.lower() in stopwords], sentences))\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            # sentences_tokens = ...\n",
    "            # YOUR CODE HERE\n",
    "            sentences_tokens = [[stemmer.stem(word) for word in sentence] for sentence in data]\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentences_prep = [\" \".join(tokens).strip() for tokens in sentences_tokens]\n",
    "        return sentences_prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194523"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    regex_list=[],\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=stemmer,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords_fp\n",
    ")\n",
    "\n",
    "data_preprocessed = text_cleaner.clean_sentences(data)\n",
    "len([w for s in data_preprocessed for w in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4a87d0c9b1f20f7e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-526fdbe57aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_cleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3770\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_preprocessed\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m22447\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdata_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'could join second afghan war broken land'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdata_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'noth misfortun disast remov brigad'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    regex_list=[],\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=stemmer,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords_fp\n",
    ")\n",
    "\n",
    "data_preprocessed = text_cleaner.clean_sentences(data)\n",
    "assert len(data_preprocessed) == 3770\n",
    "assert len([w for s in data_preprocessed for w in s.split()]) == 22447\n",
    "assert data_preprocessed[8] == 'could join second afghan war broken land'\n",
    "assert data_preprocessed[15] == 'noth misfortun disast remov brigad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f48abbdab8acc3d1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3. Movie reviews\n",
    "\n",
    "We will now use what we've learned to explore movie reviews. We will start by analysing the dataset, then we will apply the preprocessing you implemented above, and finally we will see how it affects a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d6a39b05ecd310c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.a)\n",
    "\n",
    "To get some stats on the dataset, we will start by implementing your own function to get the list of n-grams from a list of tokens. Complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43601c5f562f5867",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(data, n):\n",
    "    \"\"\"\n",
    "    Returns list of tuples for all the n-grams\n",
    "    \n",
    "    Args:\n",
    "    data - list of tokenized data (flattened)\n",
    "    n - the n in n-grams\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    from nltk import ngrams\n",
    "    return list(ngrams(data, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'actress'), ('actress', 'won'), ('won', 'the'), ('the', 'oscar')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(\"The actress won the oscar\".split(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f601c9c7d4b35092",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert ngrams(\"The actress won the oscar\".split(), 2) == [('The', 'actress'), ('actress', 'won'), ('won', 'the'), ('the', 'oscar')]\n",
    "assert ngrams(\"The actress won the oscar\".split(), 3) == [('The', 'actress', 'won'), ('actress', 'won', 'the'), ('won', 'the', 'oscar')]\n",
    "assert ngrams(\"The actress won the oscar\".split(), 4) == [('The', 'actress', 'won', 'the'), ('actress', 'won', 'the', 'oscar')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cdc8657fcd5f3c4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.b)\n",
    "\n",
    "We will now see in our dataset what are the most common n-grams. Load the data and find how many unique bi-grams, tri-grams and four-grams we have. Also, take advantage of `Counter` and `most_common()` to find the most common tri-gram. Merge together the words of the most common trigram to get one single string. (Hint: look at python's `join` function, exemplefied below when joining the full text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21e130445a725501",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text and split into full list of words\n",
    "docs = df['text']\n",
    "full_text = ' '.join([d.strip() for d in docs])\n",
    "words = full_text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-caa9453210806809",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement below the code to get the sets of unigrams, bigrams, trigrams and fourgrams, and to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-539dba0b0aabb366",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'most_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-d25179d06345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfourgrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmost_common_trigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'most_common'"
     ]
    }
   ],
   "source": [
    "# unigrams = ...\n",
    "# bigrams = ...\n",
    "# trigrams = ...\n",
    "# fourgrams = ...\n",
    "# most_common_trigram = ...\n",
    "#\n",
    "# YOUR CODE HERE\n",
    "unigrams = list(ngrams(words, 1))\n",
    "bigrams = list(ngrams(words, 2))\n",
    "trigrams = list(ngrams(words, 3))\n",
    "fourgrams = list(ngrams(words, 4))\n",
    "most_common_trigram = Counter(full_text.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-91094315aa4e5abf",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_unigrams = str(len(unigrams))\n",
    "n_bigrams = str(len(bigrams))\n",
    "n_trigrams = str(len(trigrams))\n",
    "n_fourgrams = str(len(fourgrams))\n",
    "\n",
    "print('Found {} unigrams'.format(n_unigrams))\n",
    "assert hashlib.sha256(n_unigrams.encode()).hexdigest() == '1ae2d8247d3ad491c79aed034828ba78b21e25438a6e9a61f252eb566e39e877'\n",
    "\n",
    "print('Found {} bigrams'.format(n_bigrams))\n",
    "assert hashlib.sha256(n_bigrams.encode()).hexdigest() == '7d2d487bcdf890f05578da49f574e3e8f22f7420f752071a24eb49759de5adf8'\n",
    "\n",
    "print('Found {} trigrams'.format(n_trigrams))\n",
    "assert hashlib.sha256(n_trigrams.encode()).hexdigest() == '8c54e3c7087ab053a77d56c60408fd47837081fdea817b7cc9e68f134cef969d'\n",
    "\n",
    "print('Found {} fourgrams'.format(n_fourgrams))\n",
    "assert hashlib.sha256(n_fourgrams.encode()).hexdigest() == '8df23d7f0d27298e7a7f77bdce4d15bb401098175c36514ba94b5350177b1593'\n",
    "\n",
    "print('Most common trigram is \"{}\"'.format(most_common_trigram))\n",
    "assert hashlib.sha256(most_common_trigram.encode()).hexdigest() == '28b6f04107ef3f1120975abf58ca8d08d20243beea929999b203f0add941fe16'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2477efb5c417c3d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.c)\n",
    "\n",
    "Let's now process a sample of our dataset with the previous Q2 preprocessing, and get a Bag of Words representation. Start by using your text cleaner to get a preprocessed version of this dataset.\n",
    "\n",
    "Note: if you didn't finish the text cleaner above, jump to the TF-IDF implementation directly, where you can load the BoW from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-306c1207ab2a8c1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    regex_list=[],\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=None,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords_fp\n",
    ")\n",
    "\n",
    "docs_preprocessed = text_cleaner.clean_sentences(docs[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-11aa60fa130d8eeb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can get a vocabulary, vectorize our dataset and convert it into a BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23a2f9da897ab002",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(docs):\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "def vectorize(docs):\n",
    "    vocabulary = build_vocabulary(docs)\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        vector = np.array([doc.count(word) for word in vocabulary])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return (vocabulary, vectors)\n",
    "\n",
    "def build_df(docs):\n",
    "    vocab, vectors = vectorize(docs)\n",
    "    return pd.DataFrame(vectors, columns=vocab)\n",
    "\n",
    "BoW = build_df(docs_preprocessed)\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9951e7e69d2e1c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will now implement one of TF-IDFs variation to compute from the bag of words the more relevant words. The formulation you should use is one you've learned before:\n",
    "\n",
    "$$ tfidf _{t, d} =(tf_{t,d})*(log_2{(1 + \\frac{N}{df_{t}})})  $$\n",
    "\n",
    "Implement the TF-IDF below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb6996cb41762895",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(BoW_df):\n",
    "    \"\"\"\n",
    "    Returns pandas dataframe of a tfidf representation from a BoW representation dataframe.\n",
    "\n",
    "    Args:\n",
    "    BoW_df - dataframe with document word counts (Bag of Words)\n",
    "    \"\"\"\n",
    "    # tf = (...)\n",
    "    \n",
    "    # def _idf(column):\n",
    "    #   return (...)\n",
    "    \n",
    "    # tf_idf = (...)\n",
    "    \n",
    "    # return tf_idf\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ab99eef3c657a91",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let's now apply it to our previous BoW (note: load the BoW first if you could not use your text cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffdac98448888edf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "BoW = pd.read_csv('data/imdb_sentiment_bow_sample.csv')\n",
    "BoW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5b8f988a69678772",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "relevance = tfidf(BoW)\n",
    "\n",
    "assert(math.isclose(relevance['movie'][0], 0.009717385023827248),\n",
    "       math.isclose(relevance['film'][10], 0.019778475747522496),\n",
    "       math.isclose(relevance['nice'][16], 0.010851136310680626),\n",
    "       math.isclose(relevance['good'][128], 0.00989061193998239))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-779badbcfbbc934e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.d)\n",
    "\n",
    "Now, let's use scikit-learn to get to a similar matrix and relevance numbers. Load the full processed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40fe043ee73e9a82",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_preprocessed = pd.read_csv('data/imdb_sentiment_processed.csv')\n",
    "\n",
    "# Get the processed text \n",
    "docs = df_preprocessed['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61ec1d1af055ee98",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Start by transforming your documents into a matrix of tf-idf scores using sklearn. Make use of the `CountVectorizer` and the `TfidfTransformer` provided by scikitlearn. Implement a function that provided with a list of documents returns the word term frequency matrix and the corresponding vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8300a0a7bebf4efd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_word_term_frequency_matrix(docs):\n",
    "    \"\"\"\n",
    "    Returns the matrix of word and tf-idf scores \n",
    "    \n",
    "    Args:\n",
    "    docs - list of documents in dataset\n",
    "    \"\"\"\n",
    "    # vectorizer = ...\n",
    "    # word_count_matrix = ...\n",
    "    # vocabulary = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # tfidf = ...\n",
    "    # word_term_frequency_matrix = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return (word_term_frequency_matrix, vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b548aa7912824b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now get the corresponding string of the most important word of this document (with index `321`) according to TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d00746278a646fe0",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "index = 321\n",
    "\n",
    "word_term_frequency_matrix, vocabulary = build_word_term_frequency_matrix(docs)\n",
    "\n",
    "max_word_idx = word_term_frequency_matrix[index].argmax()\n",
    "inv_vocab = {v: k for k, v in vocabulary.items()}\n",
    "most_relevant_word = inv_vocab[max_word_idx]\n",
    "\n",
    "assert(most_relevant_word == 'dull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab54cc6297c36ee2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Q3.e)\n",
    "\n",
    "Finally, let's try to classify the sentiment of these movie reviews. \n",
    "\n",
    "Build a Pipeline to classify a review as positive or negative. Use `MultinomialNB` as your final classifier, train it and get an accuracy score above 86% on the imdb validation dataset, by choosing the best set of parameters of `CountVectorizer()` and `TfidfTransformer()`, according to what we learned in Part III.\n",
    "\n",
    "Hint: Try to use more than unigrams! Also, remember what we said about stopwords and feature space size in Part III of the Learning Notebooks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca9cc0a788f3d721",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df_preprocessed, test_size=0.3, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0712ceb628e6ed6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate(train_df, validation_df):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with its \n",
    "    current accuracy in the validation set. Assume the documents are already \n",
    "    preprocessed\n",
    "    \n",
    "    Args:\n",
    "    train_df - dataframe with training docs\n",
    "    validation_df - dataframe with validation docs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline\n",
    "    # text_clf = Pipeline(...)\n",
    "    \n",
    "    # Train the classifier\n",
    "    # (...)\n",
    "\n",
    "    # predicted = (...)\n",
    "    # acc = (...)\n",
    "    \n",
    "    # return text_clf, acc\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07b6f3694941ac81",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "_, acc = train_and_validate(train_df, validation_df)\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "assert(acc >= 0.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
