{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-229243dce415caea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# BLU09 - Exercises\n",
    "\n",
    "Welcome to the exercises of the BLU09! Should you get stuck on an exercise take a look at the hints or at the learning notebook in order to get some clues. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c9c6fdf3e87a0f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hashlib import sha256\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8204f83b1061d2df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## The Goal\n",
    "In this learning unit you are going to investigate how we can extract features from our textual data to determine if an e-mail is 'spam' or 'not spam' (also known as ham). You will start by building some basic features, then go on to build more complex ones, and finally putting it all together. You should be able to have a working classifier by the end of the notebook. \n",
    "\n",
    "## The Dataset\n",
    "You are going to use a very well known Kaggle dataset for spam detection - [Kaggle Spam Collection](https://www.kaggle.com/uciml/sms-spam-collection-dataset). This is the same dataset that you looked at in the learning notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-124957b8f2212188",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/spam.csv', encoding='latin1')\n",
    "df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1,inplace=True)\n",
    "df.rename(columns={\"v1\":\"label\", \"v2\":\"message\"},inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbc68a16f01c06dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# load the medium-sized SpaCy model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb24da6fc6fda098",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of SpaCy \"Docs\" by leveraging the SpaCy pipeline\n",
    "docs = list(nlp.pipe(df[:3000].message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3964f5278d3e2e45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q1 - Text exploration with SpaCy \n",
    "\n",
    "Your team's productivity is plummeting: there is just way too many spam emails jamming everyone's inboxes. You decide to take action. In order to look cool in front of everyone and save the team from disgrace, you decide to build a spam classifier that will filter out spam emails. \n",
    "\n",
    "You decide to start simple and perform some exploration using `SpaCy`.\n",
    "\n",
    "### Q1.a) Create a simple matcher\n",
    "You suspect that the words \"FREE\", \"WIN\", and \"URGENT\" often occur in spam emails. Take advantage of SpaCy's `Matcher` to count the total number *exact* of matches of these words. Looking at the below figure should help you choose the pattern to use for this purpose.\n",
    "\n",
    "![](media/token_attributes.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1c582f5472593cf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of total exact matches of the words \"FREE\", \"WIN\", and \"URGENT\" using the SpaCy Matcher and assign it to \"count\"\n",
    "#matcher = Matcher(...)\n",
    "#\n",
    "#for ... :\n",
    "#    pattern = [...]\n",
    "#    matcher.add(...)\n",
    "#\n",
    "#count =0\n",
    "#for doc in docs:\n",
    "#    matches = ...\n",
    "#    count += ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"ORTH\": 'FREE'}] \n",
    "pattern2 = [{\"ORTH\": 'WIN'}]\n",
    "pattern3 = [{\"ORTH\": 'URGENT'}]\n",
    "matcher.add('url', None, pattern1, pattern2, pattern3)\n",
    "count = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        count.append(doc[start:end])\n",
    "    \n",
    "\n",
    "count = len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c4c03bc54acfdc0",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "count_hash = '7b1a278f5abe8e9da907fc9c29dfd432d60dc76e17b0fabab659d2a508bc65c4'\n",
    "assert sha256(str(count).encode()).hexdigest() == count_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90610064b252d533",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q1.b) Extract URLs\n",
    "\n",
    "You also suspect that spam messages have many URLs. Build a matcher to extract all URLs from the text and store the URLs (and only the URLs!) in a list. Assign the result to `url_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e7783e89b853ce6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Extract all the URLs in the text and store them in a list called url_list\n",
    "#matcher = Matcher(...)\n",
    "#\n",
    "#url_list = []\n",
    "#\n",
    "#pattern = [...]\n",
    "#matcher.add(...)\n",
    "#\n",
    "#for doc in docs:\n",
    "#    matches = ...\n",
    "#    for ... in matches:\n",
    "#        url_list.append(...)\n",
    "        \n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "url_list = []\n",
    "pattern = [{'LIKE_URL':True}]\n",
    "matcher.add('url', None, pattern)\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        url_list.append(doc[start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-790c430e04cb0f1e",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "list_hash = '141ad9ecb8c7ee6199cd399cf154f13ca99858aa6a71ff8a227dccd28f909350'\n",
    "assert len(url_list) == 88\n",
    "assert sha256(','.join(map(str, url_list)).encode()).hexdigest() == list_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-880531b38c9b67b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q1.c) Extract Part of Speech features\n",
    "\n",
    "You also think that spam messages may have many verbs (luring you to take action), and so you decide to extract the verbs from the text. \n",
    "\n",
    "To help you, here's the list of PoS available in SpaCy:\n",
    "\n",
    "![](media/pos_helper.png)\n",
    "\n",
    "To complete this exercise you should build a matcher to extract verbs. Use this matcher to create a list containing the number of verbs for each document. Store the verb counts in a list called `verb_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64683287d0596aeb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Store the verb counts (according to SpaCy's matcher) per document in a list called verb_counts\n",
    "#Hint: using the function \"len(...)\" might help\n",
    "#\n",
    "#matcher = ...\n",
    "#pattern = [...]\n",
    "#matcher.add(...)\n",
    "#\n",
    "#verb_counts = []\n",
    "#for doc in docs:\n",
    "#    matches = ...\n",
    "#    verb_counts.append(...)\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS': 'VERB'}]\n",
    "matcher.add('verb', None, pattern)\n",
    "verb_counts = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    verb_counts.append(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7b15b3c21fe74f5",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "hash_count = '073a8f81f495d87e3ebf393c8db29d2f8bb0767a18aff2d3aa55ffd10e8787bf'\n",
    "assert sha256(str(sum(verb_counts)).encode()).hexdigest() == hash_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.d) Extract entities\n",
    "\n",
    "You also think it would be useful to extract some Organizations from the text in order to check for any re-occurring patterns.\n",
    "\n",
    "Build a `Matcher` to match organizations in the text and extract the top 5 most common ones. Assign them to `most_common_ents`.\n",
    "\n",
    "*hint: Use [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) to extract the most common elements (check the most_common(n) method). You will need to feed it strings (not SpaCy spans)*\n",
    "\n",
    "*note: in a real-case scenario we would perform some text preprocessing first and build a better entity recognizer, but let us not worry about that here*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c82f29c4da571fd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Build a matcher to extract the organization-type entities from the text and assign them to most_common_ents\n",
    "#\n",
    "#matcher = ...\n",
    "#\n",
    "#pattern = [...]\n",
    "#matcher.add(...)\n",
    "# \n",
    "#...\n",
    "#\n",
    "# most_common_ents = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'ENT_TYPE':'ORG'}]\n",
    "matcher.add('pat', None, pattern)\n",
    "url_list = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        url_list.append(doc[start:end])\n",
    "\n",
    "labels = [x.label_ for x in url_list]\n",
    "items = [i.text for i in url_list]\n",
    "\n",
    "most_common_ents = Counter(items).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ae53a5d4d611796a",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ent_hash = \"1d45ae99abcc02002be90eabecf61d0ce0613d1de5f0c37ddd7bbbd7e8198cf5\"  # sha256('Nokia')\n",
    "assert len(most_common_ents) == 5\n",
    "assert ent_hash in (sha256(ent.encode()).hexdigest() for ent, count in most_common_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45e34f159eafa6a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q2 - Extracting features\n",
    "\n",
    "You decide to start extracting more complex features in the hope of extracting more information from the text.\n",
    "Let us now work with the full Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = list(nlp.pipe(df['message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.a) Create \"complex\" patterns\n",
    "You suspect that spammers write emails that compel you to take action by talking about things which are great. So, you decide to investigate the ocurrences of an adjective followed by an entity.\n",
    "\n",
    "Build a `Matcher` that matches all occurrences of an adjective followed by an organization and store the 5 most common ocurrences in a list named `most_common_adj_ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68823353c4240178",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Build a `Matcher` that matches all ocurrences of an adjective followed by\n",
    "#and organization and store the 5 most common ocurrences in a list named most_common_adj_ents\n",
    "#\n",
    "#matcher = ...\n",
    "#\n",
    "#pattern = [..., ...]\n",
    "#matcher.add(...)\n",
    "# \n",
    "#...\n",
    "#\n",
    "#most_common_adj_ents = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 =   [{'POS': 'ADJ'}, {'ENT_TYPE':'ORG'}]\n",
    "matcher.add('b', None, pattern1)\n",
    "url_list = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        url_list.append(doc[start:end])\n",
    "        \n",
    "url_list = [x.text for x in url_list]\n",
    "        \n",
    "most_common_adj_ents = Counter(url_list).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bc7be7e8fca147d1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "adj_ent_hash = \"4dbad11fe79c962e80f0ad3dc3ef788eaa4b7412c6c99f1d1fe78586486e4afe\"  # sha256('top Sony')\n",
    "assert len(most_common_adj_ents) == 5\n",
    "assert adj_ent_hash in (sha256(adj_ent.encode()).hexdigest() for adj_ent, count in most_common_adj_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0a2a5c45b3c130c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q2.b) Create Numerical Features\n",
    "\n",
    "You start thinking what features could actually be useful for solving your problem. One possible factor that may help is to know the number of adjectives and verbs used, the number of entities, and the length of the messages. \n",
    "\n",
    "Add extra fields to the `df` dataframe with the count of the number of adjectives and verbs, entities (all types), and the length of the document. For this consider the adjectives, verbs, and entities as those identified by SpaCy and the length of the text as the count of characters. \n",
    "\n",
    "*note: the number of verbs and adjectives should be summed in a single variable*\n",
    "\n",
    "Assign the number of verbs and adjectives to a new column called `n_verb_adjs`, the number of entities to a column called `n_ents`, and the length of the message to a column called `len_message`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bab0e5a3f8f8cbc7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#Assign the number of verbs and adjectives to a new column called n_verbs_adjs,\n",
    "#the number of entities to a column called n_ents,\n",
    "#and the lenght of the message to a column called len_message\n",
    "#\n",
    "# Hint: you can iterate over the tokens in Spacy doc to inspect them \n",
    "# for doc in all docs:\n",
    "#    print(doc.ents)\n",
    "#    for token in doc:\n",
    "#        print(token.pos_)\n",
    "\n",
    "#n_ents = []\n",
    "#n_verb_adjs = []\n",
    "#len_message = []\n",
    "#\n",
    "#for doc in all_docs:\n",
    "#    count_ents = 0\n",
    "#    count_verb_adjs = 0\n",
    "#    ...\n",
    "#    n_ents.append(count_ents)\n",
    "#    n_verb_adjs.append(count_verb_adjs)\n",
    "#    len_message.append(...)\n",
    "#    \n",
    "#df['n_verb_adjs'] = n_verb_adjs\n",
    "#df['n_ents'] = n_ents\n",
    "#df['len_message'] = len_message\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "from operator import add\n",
    "\n",
    "n_ents = []\n",
    "n_verb_adjs = []\n",
    "len_message = []\n",
    "   \n",
    "matcher = Matcher(nlp.vocab)\n",
    "verb_pattern = [{'POS': 'VERB'}]\n",
    "matcher.add('verb', None, verb_pattern)\n",
    "verb_counts = []\n",
    "for i, doc in enumerate(all_docs):\n",
    "        n_ents.append(len(doc.ents))\n",
    "for doc in all_docs:\n",
    "    matches = matcher(doc)\n",
    "    verb_counts.append(len(matches))\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "adj_pattern = [{'POS': 'ADJ'}]\n",
    "matcher.add('adj', None, adj_pattern)\n",
    "adj_counts = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    matches = matcher(doc)\n",
    "    adj_counts.append(len(matches))\n",
    "\n",
    "n_verb_adjs = list(map(add, verb_counts, adj_counts))\n",
    "for doc in all_docs:\n",
    "    len_message.append(len(doc))\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "df['n_verb_adjs'] = n_verb_adjs\n",
    "df['n_ents'] = n_ents\n",
    "df['len_message'] = len_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>n_verb_adjs</th>\n",
       "      <th>n_ents</th>\n",
       "      <th>len_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  n_verb_adjs  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...            6   \n",
       "1   ham                      Ok lar... Joking wif u oni...            1   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...            6   \n",
       "3   ham  U dun say so early hor... U c already then say...            2   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...            4   \n",
       "\n",
       "   n_ents  len_message  \n",
       "0       1           24  \n",
       "1       0            8  \n",
       "2       6           31  \n",
       "3       0           13  \n",
       "4       0           15  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-03dbc9fa2ea23663",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert all(col in df.columns for col in ('n_verb_adjs', 'n_ents', 'len_message'))\n",
    "assert np.allclose(df.n_verb_adjs.sum(), 23979, 20)\n",
    "assert np.allclose(df.n_ents.sum(), 5537, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-923d795a48aa24c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Q3 - Pipelines and Feature Unions\n",
    "It is now time for you to leverage on your newly built features and construct pipelines that can be fed to a classifier. You decide to use a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) as you hear from industry experts it tends to work well for text classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['message'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c8ff5d97624ac9b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7084694ab285eb01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "    \n",
    "def get_accuracy(feats, train_data, test_data):\n",
    "    \"\"\"\n",
    "    Return the accuracy on the test_data by using a RandomForestClassifier trained on the \n",
    "    train_data with the features described by feats\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features',feats),\n",
    "        ('classifier', RandomForestClassifier(random_state = 42, n_estimators=10)),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(train_data, train_data.label)\n",
    "\n",
    "    preds = pipeline.predict(test_data)\n",
    "    accuracy = np.mean(preds == test_data.label)\n",
    "    \n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ba44a8653efe95",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q3.a) Build a Feature Union\n",
    "You hypothesize that combining the text and numerical features could help you build a strong classifier. \n",
    "\n",
    "Use `FeatureUnion` to join the text features extracted from a standard sklearn `TfidfVectorizer` (with $ngram\\_range=(1,2)$) and the numeric feature of the length of the messages scaled to zero mean and unit variance *[hint](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)*. Assign them to a variable named `feats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0848e748dc61721f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#text_pipe = Pipeline(...)\n",
    "#len_pipe =  Pipeline(...)\n",
    "#feats = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "text_pipe =  Pipeline([\n",
    "                ('selector', TextSelector(\"message\")),\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,2)))\n",
    "            ])\n",
    "\n",
    "len_pipe =  Pipeline([\n",
    "                ('selector', NumberSelector(\"length\")),\n",
    "                ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))\n",
    "            ])\n",
    "\n",
    "\n",
    "feats = FeatureUnion([('text_pipe', text_pipe), \n",
    "                      ('len_pipe', len_pipe)])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-80971dd7f21ec905",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert any(isinstance(obj, Selector) for obj in feats.transformer_list[0][1])\n",
    "assert any(isinstance(obj, TfidfVectorizer) for obj in feats.transformer_list[0][1])\n",
    "assert np.allclose(get_accuracy(feats, train_data, test_data), 0.9668, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f4ecd3a08b31cb9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q3.b) Add more features\n",
    "You decide to try adding the number of verbs and adjectives to your features to see if they can improve the performance of your classifier. \n",
    "\n",
    "Add the number of verbs and adjs `n_verb_adjs` that you computed in Q2.b to your features. Assign your features to `feats_v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ef7216ae6023fdf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NumberSelector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-5e334e38fffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m verbs = Pipeline([\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'selector'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumberSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'verbs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'standard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NumberSelector' is not defined"
     ]
    }
   ],
   "source": [
    "#verbs = Pipeline(...)\n",
    "#...\n",
    "#feats_v2 = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "verbs = Pipeline([\n",
    "                ('selector', NumberSelector(key='verbs')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2726b142fbec4f14",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = get_accuracy(feats_v2, train_data, test_data)\n",
    "assert np.allclose(accuracy, 0.9704, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1aa91b08408579d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Q3.c) Add the entities feature\n",
    "You try to improve your model even further by including the number of entities `n_ents` feature that you created in Q2.b above. \n",
    "\n",
    "Add the number of entities to your features and assign the result to `feats_v3` (**no need to scale** the features this time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f5770ba6afa04742",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#...\n",
    "#feats_v3 = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8289104bef520ea7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = get_accuracy(feats_v3, train_data, test_data)\n",
    "assert np.allclose(accuracy, 0.9659, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You realize that your accuracy actually decreased, which reminds you that more features does not necessarily mean better results.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "You realize you can get fairly high accuracy on the spam problem using a fairly simple solution. You know there are many things you could improve and many further paths you could choose in order to try to take your classifier to the next level, but you decide to leave that challenge for another day. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
