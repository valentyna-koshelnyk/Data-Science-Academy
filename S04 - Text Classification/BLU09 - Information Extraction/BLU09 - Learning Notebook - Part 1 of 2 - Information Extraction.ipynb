{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Information Extraction\n",
    "\n",
    "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. Information Extraction may be presented in three subtasks:\n",
    "\n",
    "* **Named Entity Recognition**, retrieve entities (like persons, location, etc.) in the text. \n",
    "* **Relation Extraction**, find the relation between two entities in the text.\n",
    "* **Template Filling**, find the correct entity to fill a certain template, for instance.\n",
    "\n",
    "In this BLU we are going to learn some of the basic techniques to extract specific (pre-specified) information from textual sources. From the three specified task, we are going to **focus on the task of named-entity recognition (NER)** where our objective is to **retrieve all the mentions** of entities like persons, locations, time, among others. The other two are mentioned for the sake of completeness and you should definitely research more about them, specially if you're eager to learn more about NLP.\n",
    "\n",
    "![robot entities](./media/robot_entities.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work in a corpus containing forum discussions. We extracted a sample from Reddit for this use. For more interesting examples, you may find more textual data available at https://files.pushshift.io/reddit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read 1000 documents\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('./datasets/sample_data.json') as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        docs.append(entry['body'])\n",
    "        \n",
    "print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction with Regular Expressions\n",
    "\n",
    "In BLU7, we became pros of regular expressions. We're going to try to use them to our task of recognizing entities. Take a moment to think about all the possibilities of Entities that we can find in a text. Do you think such a task will be achievable using only regular expressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regex](./media/regex.gif \"regex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, let's say that your boss asked you to retrieve all the **dates** mentioned in our sample corpus. We learned in BLU7 that it is easy to use a regular expression for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14/09/30', '7/12/2007', '4/16/2007', '3/27/2007', '2/28/2007']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find all possible dates in the format xx/xx/xxxx\n",
    "data = ' '.join(docs)\n",
    "re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks like it's going to be a breeze. However, now your boss decides to ask you to retrieve all the **country names** which appear in the corpus instead. \n",
    "\n",
    "One possible approach is to retrieve a list of all countries that exist and look for the occurence of such elements in the corpus. Let's try that, shall we?\n",
    "\n",
    "![alt text](./media/countries_meme.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'American Samoa',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Anguilla',\n",
       " 'Antarctic Lands',\n",
       " 'Antarctica',\n",
       " 'Antigua',\n",
       " 'Antigua and Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia',\n",
       " 'Aruba',\n",
       " 'Ashmore Islands',\n",
       " 'Ashmore and Cartier Islands',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Azerbaijan',\n",
       " 'Bahamas',\n",
       " 'Bahrain',\n",
       " 'Baker Island',\n",
       " 'Bangladesh',\n",
       " 'Barbados',\n",
       " 'Barbuda',\n",
       " 'Bassas da India',\n",
       " 'Belarus',\n",
       " 'Belgium',\n",
       " 'Belize',\n",
       " 'Benin',\n",
       " 'Bermuda',\n",
       " 'Bhutan',\n",
       " 'Bolivia',\n",
       " 'Borneo',\n",
       " 'Bosnia',\n",
       " 'Bosnia Herzegovina',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Botswana',\n",
       " 'Bouvet Island',\n",
       " 'Brazil',\n",
       " 'Britain',\n",
       " 'British Indian Ocean Territory',\n",
       " 'British Virgin Islands',\n",
       " 'Brunei',\n",
       " 'Bulgaria',\n",
       " 'Burkina Faso',\n",
       " 'Burma',\n",
       " 'Burundi',\n",
       " 'Caicos Islands',\n",
       " 'Cambodia',\n",
       " 'Cameroon',\n",
       " 'Canada',\n",
       " 'Cape Verde',\n",
       " 'Cartier Islands',\n",
       " 'Cayman Islands',\n",
       " 'Central African Republic',\n",
       " 'Chad',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Christmas Island',\n",
       " 'Clipperton',\n",
       " 'Clipperton Island',\n",
       " 'Cocos',\n",
       " 'Colombia',\n",
       " 'Comoros',\n",
       " 'Congo',\n",
       " 'Democratic Republic of the Congo',\n",
       " 'Republic of the Congo',\n",
       " 'Cook Islands',\n",
       " 'Coral Sea Islands',\n",
       " 'Costa Rica',\n",
       " \"Cote D 'Ivoire\",\n",
       " \"Cote d 'Ivoire\",\n",
       " 'Croatia',\n",
       " 'Cuba',\n",
       " 'Cyprus',\n",
       " 'Czech Republic',\n",
       " 'Czechoslovakia',\n",
       " 'Democratic Republic of the Congo',\n",
       " 'Denmark',\n",
       " 'Djibouti',\n",
       " 'Dominica',\n",
       " 'Dominican Republic',\n",
       " 'East Timor',\n",
       " 'Ecuador',\n",
       " 'Egypt',\n",
       " 'El Salvador',\n",
       " 'England',\n",
       " 'Equatorial Guinea',\n",
       " 'Eritrea',\n",
       " 'Estonia',\n",
       " 'Ethiopia',\n",
       " 'Europa Island',\n",
       " 'European Union',\n",
       " 'Falkland Islands',\n",
       " 'Faroe Islands',\n",
       " 'Federated States of Micronesia',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'Former Yugoslav Republic of Macedonia',\n",
       " 'France',\n",
       " 'French Guiana',\n",
       " 'French Polynesia',\n",
       " 'French Southern',\n",
       " 'French Southern and Antarctic Lands',\n",
       " 'FRG',\n",
       " 'Gabon',\n",
       " 'Gambia',\n",
       " 'Georgia',\n",
       " 'Germany',\n",
       " 'Ghana',\n",
       " 'Gibraltar',\n",
       " 'Glorioso',\n",
       " 'Glorioso Islands',\n",
       " 'Greece',\n",
       " 'Greenland',\n",
       " 'Grenadan',\n",
       " 'Guadeloupe',\n",
       " 'Guam',\n",
       " 'Guatemala',\n",
       " 'Guernsey',\n",
       " 'Guinea',\n",
       " 'Guinea-Bissau',\n",
       " 'Guyana',\n",
       " 'Haiti',\n",
       " 'Heard Island',\n",
       " 'Heard Island and McDonald Islands',\n",
       " 'Herzegovina',\n",
       " 'Holland',\n",
       " 'Holy See',\n",
       " 'Honduras',\n",
       " 'Hong Kong',\n",
       " 'Howland Island',\n",
       " 'Hungary',\n",
       " 'Iceland',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " 'Iran',\n",
       " 'Iraq',\n",
       " 'Ireland',\n",
       " 'Islas Malvinas',\n",
       " 'Isle of Man',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Ivory Coast',\n",
       " 'Jamaica',\n",
       " 'Jan Mayen',\n",
       " 'Japan',\n",
       " 'Jarvis Island',\n",
       " 'Jersey',\n",
       " 'Johnston Atoll',\n",
       " 'Jordan',\n",
       " 'Juan de Nova Island',\n",
       " 'Kazakhstan',\n",
       " 'Keeling Islands',\n",
       " 'Kenya',\n",
       " 'Kingman Reef',\n",
       " 'Kiribati',\n",
       " 'Korea',\n",
       " 'Kuwait',\n",
       " 'Kyrgyzstan',\n",
       " 'Laos',\n",
       " 'Latvia',\n",
       " 'Lebanon',\n",
       " 'Lesotho',\n",
       " 'Liberia',\n",
       " 'Libya',\n",
       " 'Liechtenstein',\n",
       " 'Lithuania',\n",
       " 'Luxembourg',\n",
       " 'Macau',\n",
       " 'Macedonia',\n",
       " 'Madagascar',\n",
       " 'Malawi',\n",
       " 'Malaysia',\n",
       " 'Maldives',\n",
       " 'Mali',\n",
       " 'Malta',\n",
       " 'Man , Isle of',\n",
       " 'Marshall Islands',\n",
       " 'Martinique',\n",
       " 'Mauritania',\n",
       " 'Mauritius',\n",
       " 'Mayotte',\n",
       " 'McDonald Islands',\n",
       " 'Mexico',\n",
       " 'Micronesia',\n",
       " 'Midway Islands',\n",
       " 'Moldova',\n",
       " 'Monaco',\n",
       " 'Mongolia',\n",
       " 'Montenegro',\n",
       " 'Montserrat',\n",
       " 'Morocco',\n",
       " 'Mozambique',\n",
       " 'Myanmar',\n",
       " 'Namibia',\n",
       " 'Nauru',\n",
       " 'Navassa Island',\n",
       " 'Nepal',\n",
       " 'Netherlands',\n",
       " 'Netherlands Antilles',\n",
       " 'Nevis',\n",
       " 'New Caledonia',\n",
       " 'New Guinea',\n",
       " 'New Zealand',\n",
       " 'Nicaragua',\n",
       " 'Niger',\n",
       " 'Nigeria',\n",
       " 'Niue',\n",
       " 'Norfolk Island',\n",
       " 'North Korea',\n",
       " 'North Vietnam',\n",
       " 'Northern Mariana Islands',\n",
       " 'Norway',\n",
       " 'Oman',\n",
       " 'PRC',\n",
       " 'Pakistan',\n",
       " 'Palau',\n",
       " 'Palestine',\n",
       " 'Palmyra Atoll',\n",
       " 'Panama',\n",
       " 'Papua',\n",
       " 'Papua New Guinea',\n",
       " 'Paracel Islands',\n",
       " 'Paraguay',\n",
       " 'People Republic of China',\n",
       " 'Persia',\n",
       " 'Peru',\n",
       " 'Philippines',\n",
       " 'Pitcairn Islands',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Puerto Rico',\n",
       " 'Qatar',\n",
       " 'Republic of the Congo',\n",
       " 'Republic of Chile',\n",
       " 'Reunion',\n",
       " 'Romania',\n",
       " 'Russia',\n",
       " 'Rwanda',\n",
       " 'Saint Helena',\n",
       " 'Saint Kitts',\n",
       " 'Saint Kitts and Nevis',\n",
       " 'Saint Lucia',\n",
       " 'Saint Pierre and Miquelon',\n",
       " 'Saint Vincent and the Grenadines',\n",
       " 'Samoa',\n",
       " 'San Marino',\n",
       " 'Sao Tome and Principe',\n",
       " 'Saudi Arabia',\n",
       " 'Senegal',\n",
       " 'Serbia',\n",
       " 'Serbia and Montenegro',\n",
       " 'Seychelles',\n",
       " 'Sierra Leone',\n",
       " 'Singapore',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'Solomon Islands',\n",
       " 'Somalia',\n",
       " 'South Africa',\n",
       " 'South Georgia',\n",
       " 'South Georgia and the South Sandwich Islands',\n",
       " 'South Korea',\n",
       " 'South Sandwich Isklands',\n",
       " 'South Sandwich Islands',\n",
       " 'South Vietnam',\n",
       " 'Soviet',\n",
       " 'soviet',\n",
       " 'Soviet Union',\n",
       " 'Spain',\n",
       " 'Spratly Islands',\n",
       " 'Sri Lanka',\n",
       " 'Sudan',\n",
       " 'Suriname',\n",
       " 'Svalbard',\n",
       " 'Swaziland',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'Syria',\n",
       " 'Taiwan',\n",
       " 'Tajikistan',\n",
       " 'Tanzania',\n",
       " 'Thailand',\n",
       " 'Tibet',\n",
       " 'Tobago',\n",
       " 'Togo',\n",
       " 'Tokelau',\n",
       " 'Tonga',\n",
       " 'Trinidad',\n",
       " 'Trinidad and Tobago',\n",
       " 'Tromelin Island',\n",
       " 'Tunisia',\n",
       " 'Turkey',\n",
       " 'Turkmenistan',\n",
       " 'Turks',\n",
       " 'Turks and Caicos Islands',\n",
       " 'Tuvalu',\n",
       " 'U.K.',\n",
       " 'U.S.',\n",
       " 'U.k.',\n",
       " 'UK',\n",
       " 'US',\n",
       " 'USA',\n",
       " 'U.S.A.',\n",
       " 'USSR',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'United Arab Emirates',\n",
       " 'United Kingdom',\n",
       " 'United States',\n",
       " 'United States of America',\n",
       " 'Uruguay',\n",
       " 'Uzbekistan',\n",
       " 'Vanuatu',\n",
       " 'Vatican City',\n",
       " 'Vatican',\n",
       " 'Venezuela',\n",
       " 'Vietnam',\n",
       " 'Virgin Islands',\n",
       " 'Wake Island',\n",
       " 'Wallis and Futuna',\n",
       " 'Western Sahara',\n",
       " 'Yemen',\n",
       " 'Yugoslav Republic of Macedonia',\n",
       " 'Yugoslavia',\n",
       " 'Zaire',\n",
       " 'Zambia',\n",
       " 'Zimbabwe',\n",
       " 'Wales']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = []\n",
    "with open('./datasets/countries.txt') as fp:\n",
    "    for line in fp:\n",
    "        countries.append(line.rstrip())\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use again regular expressions for this. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('us', 763, 765)\n",
      "('United States', 827, 840)\n",
      "('UK', 6971, 6973)\n",
      "('US', 7000, 7002)\n",
      "('Puerto rico', 8026, 8037)\n",
      "('us', 8638, 8640)\n",
      "('France', 19815, 19821)\n",
      "('us', 21563, 21565)\n",
      "('Puerto Rico', 27659, 27670)\n",
      "('Puerto Rico', 27754, 27765)\n",
      "('US', 28101, 28103)\n",
      "('Canada', 29439, 29445)\n",
      "('USA', 32880, 32883)\n",
      "('Norway', 34749, 34755)\n",
      "('Korea', 34837, 34842)\n",
      "('USA', 35738, 35741)\n",
      "('United States', 41060, 41073)\n",
      "('us', 42290, 42292)\n",
      "('us', 42403, 42405)\n",
      "('Soviet', 44563, 44569)\n",
      "('us', 49625, 49627)\n",
      "('Chad', 51352, 51356)\n"
     ]
    }
   ],
   "source": [
    "# Sort country list by length. This is important to match longer spans before short \n",
    "# ones (like in 'Papua New Guinea' vs. 'Papua')\n",
    "countries.sort(key=len, reverse=True)\n",
    "\n",
    "# Make a regex to recognize all possible names.\n",
    "# '|' creates the or operation in regex\n",
    "# \\b means word boundaries (punctuation or white spaces)\n",
    "# re.escape is used to escape regex operators like '.'    \n",
    "countries_regex = r'\\b(' + '|'.join([re.escape(c) for c in countries]) + r')\\b'\n",
    "\n",
    "# finditer is similar to findall\n",
    "# the flag re.I means to ignore casing (accept both lowercase and uppercase letters as the same)\n",
    "for i, m in enumerate(re.finditer(countries_regex, data, flags=re.I)):\n",
    "    print( (m.group(), m.start(), m.end()) )\n",
    "    # just show the first 20\n",
    "    if i > 20:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this approach working?**\n",
    "\n",
    "It seems like the word **'us'**, for example, has caused some confusion. It could be the country _U.S._, or just the pronoun _us_. In this case, just comparing the word form we are not able to disambiguate the two forms. We will need either more **context** or more **linguistic information** and regular expression won't give us none of that.\n",
    "\n",
    "Luckily, you already know an NLP library which can provide you the correct information to disambiguate the word 'us'. In the next examples, we will use SpaCy as our NLP toolkit to give us just that.\n",
    "\n",
    "## Deeper look in information extraction using SpaCy\n",
    "![Spacy](./media/spacy.jpg)\n",
    "\n",
    "If you remember BLU8, we used SpaCy to understand word vectors (aka word embeddings). We will make use of the medium sized SpaCy english model once again. In case you haven't downloaded it yet, here's the command once again:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "    \n",
    "But of course we could have used any english model (en_core_web_sm, en_core_web_md, en_core_web_lg) provided by SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are disabling the synctatic parser from pipeline to improve speed.\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SpaCy, we will process the documents with the complete NLP pipeline using [pipe](https://spacy.io/usage/processing-pipelines). This means that `pipe` will process our text, tokenize it and extract information from it using all the CPU cores from our machine. Concretely, it will Part-of-Speech tag (more on that later), parse and extract entities.\n",
    "\n",
    "We won't get into details on how SpaCy does this -- what matters is that it uses fast machine learning models with good enough accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "# In this step, SpaCy performs the NLP pipeline for all the docs, so it may take a while.\n",
    "docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to do NER (Named Entity Extraction) in a piece of text. We can get an example sentence from our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR Tolkien. Gandalf, Aragorn, Frodo, Bilbo Baggins, Gollum...\n"
     ]
    }
   ],
   "source": [
    "example = docs[631]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SpaCy, it's really easy to extract entities - we can simply use `.ents` in our previously processed text, and SpaCy will use its built-in model to get the entities present in the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR Tolkien 0 11 PERSON\n",
      "Gandalf 13 20 PERSON\n",
      "Aragorn 22 29 PERSON\n",
      "Frodo 31 36 PERSON\n",
      "Bilbo Baggins 38 51 PERSON\n",
      "Gollum 53 59 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in example.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example sentence, SpaCy correctly labels all these LOTR characters with the Person entity. You could further argue that Gandalf is a wizard and Frodo/Bilbo are hobbits, but let's not penalize SpaCy on that one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our text is processed and we know how to get entities, let's build a `Matcher` in SpaCy.\n",
    "\n",
    "A `Matcher` is SpaCy's version of a regular expression - it searches for patterns in your text, according to the rules you give it. However, it is much more powerful since it has access to the outputs of the aforementioned NLP pipeline. That means we can search patterns that include certain entities or Part-of-Speech tags. \n",
    "\n",
    "In this `Matcher` we will define templates which we will use later to match elements in the text (thus using it to do information extraction). The `Matcher` is initialized using the vocabulary object, which must be shared with the documents the matcher will operate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a similar matcher as we did above with regular expressions. We are going to get each country name and add it as a pattern to the `matcher`. To add a pattern, we can simply use `.add()`. It receives:\n",
    "\n",
    "- an ID (the name we want to give our pattern)\n",
    "- a callable function that is called when there is a match (we're not going to use anything)\n",
    "- the pattern itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Build a pattern from the country name. For example: United States -> [{'LOWER': 'united'}, {'LOWER': 'states'}]\n",
    "    # LOWER means to match the words in the lowercased token.\n",
    "    pattern = [{'LOWER': c.lower()} for c in country.split()]\n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 2 us\n",
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "69 50 51 us\n",
      "146 4 5 France\n",
      "167 29 30 us\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 78 79 Norway\n",
      "263 101 102 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "320 35 36 us\n",
      "320 58 59 us\n",
      "335 38 39 Soviet\n",
      "335 38 39 Soviet\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "# for screen economy, let's just show the matches for the first 400 documents.\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, in order to disambiguate the retrieval of 'U.S.' vs 'us' we need to add more linguistic information to the `matcher`. Let's play with Part of Speech (PoS).\n",
    "\n",
    "## But what is Part-of-Speech?\n",
    "\n",
    "If you remember from your language classes, you could categorize words in a sentence according to the role they have in it. In NLP, we call this Part of Speech tags. For the English language, common PoS tags are: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection.\n",
    "\n",
    "SpaCy adopts the Universal PoS tagset where any language has a common subset of PoS defined. The list of all possible values can be consulted [here](https://spacy.io/api/annotation#pos-tagging).\n",
    "\n",
    "In this case, we are interested in matching the country names that were tagged as **Proper Nouns** ('PROPN' tag obtained from the tagset list).\n",
    "\n",
    "![Pronoun meme](./media/pronoun.jpg)\n",
    "\n",
    "In SpaCy, just as entities of a document are inside `doc.ents`, for each token of a document we can find its assigned POS tag by using `.pos_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `Matcher` is pretty smart, so we only really need to add to a `'POS'` entry in the pattern dictionary and the tag we are looking for as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new matcher instance\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for country in countries:\n",
    "    # same as before, but now with one more restriction: the Part-of-speech should be a Pronoun.\n",
    "    pattern = [{'LOWER': c.lower(), 'POS': 'PROPN'} for c in country.split()]    \n",
    "    matcher.add(country, None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JRR PROPN\n",
      "Tolkien PROPN\n",
      ". PUNCT\n",
      "Gandalf PROPN\n",
      ", PUNCT\n",
      "Aragorn PROPN\n",
      ", PUNCT\n",
      "Frodo PROPN\n",
      ", PUNCT\n",
      "Bilbo PROPN\n",
      "Baggins PROPN\n",
      ", PUNCT\n",
      "Gollum PROPN\n",
      "... PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in example:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "146 4 5 France\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 78 79 Norway\n",
      "263 101 102 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly the PoS tagger is based on a machine learning method, so it is prone to errors. Notice how it causes _Puerto rico_ of document 64 to be out of this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting using complex patterns\n",
    "\n",
    "Let's now look into other types of information extraction methods which use complex structures. For example, let's say we want to extract places. Usually, places come up in text in structures similar to:\n",
    "\n",
    "* go to xx\n",
    "* went from xxx\n",
    "* going to xx\n",
    "\n",
    "**Note**: Notice that such patterns could be interesting to the task of relation extraction we mentioned in the intro. But that's something we will leave up to you to look further into.\n",
    "\n",
    "In order to build a SpaCy pattern for the proposed sentence structure, we are going to use the lemma word 'go' (remember lemmatization from BLU07? We can do this in SpaCy pretty easily as well!), which is invariant for all possible verb inflexitions, a preposition (POS tag name - ADP) and a proper noun (POS tag name - PROPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'LEMMA': 'go'}, {'POS': 'ADP'}, {'POS': 'PROPN'}]\n",
    "matcher.add('LOC', None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 27 goes to GTA\n",
      "246 249 going to Osaka\n",
      "81 84 gone to Irvine\n",
      "91 94 going with Robbie\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        print(start, end, span_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sure aren't all the locations that are present in our corpus! Not what we expected then :( \n",
    "\n",
    "Once again, we are finding out that it is very difficult to build patterns to match these type of ocurrences in the text. Addressing all possible patterns for person, location, etc. this way is very inneficient and difficult. \n",
    "\n",
    "Another possible way to go is to annotate examples in a corpus. We can train machine learning systems to automatically extract patterns from annotated corpora. Such class of machine learning methods are known as sequencial labeling and the most famous approaches are [CRFs](https://people.cs.umass.edu/~wallach/technical_reports/wallach04conditional.pdf) and [Seq2seq](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).\n",
    "\n",
    "Fortunately, as explained above, Spacy already contains pre-trained models for standard named-entities. Besides _Person_ (PER) entities like _Bilbo_ and _Organization_ (ORG) entities like _PayPal_ , we can also extract _Location_ entities with the code GPE!\n",
    "\n",
    "Let's try to extract all Locations using the built-in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 November 3 ‚Äì 5 0 14\n",
      "1 the Portland Expo Center 18 42\n",
      "3 New York 0 8\n",
      "3 North America 57 70\n",
      "11 Soraka 10 16\n",
      "12 143413934| &gt 0 14\n",
      "12 United States Anonymous 16 39\n",
      "12 the Democratic Party 95 115\n",
      "12 SJW 175 178\n",
      "15 Nova 0 4\n",
      "20 8 months or so 268 282\n",
      "23 Isoprop 0 7\n",
      "25 Russians 71 79\n",
      "25 Wikileaks 92 101\n",
      "27 UCCI 35 39\n",
      "27 a week 97 103\n",
      "27 days 363 367\n",
      "27 24-48 hour 481 491\n",
      "30 Portland 5 13\n",
      "30 the weekends 151 163\n",
      "30 Portland 402 410\n",
      "30 Some days 412 421\n",
      "33 Cod BO2 53 60\n",
      "34 0-3 0 3\n",
      "35 1](/r/AskReddit/wiki 72 92\n",
      "37 Shangela 7 15\n",
      "38 two 0 3\n",
      "40 CL 22 24\n",
      "40 Accord 265 271\n",
      "40 first 300 305\n",
      "47 two 22 25\n",
      "49 Breanne at Stone Salon 0 22\n",
      "49 Hoover 26 32\n",
      "49 Facebook 161 169\n",
      "50 Brad 0 4\n",
      "50 Lonzo Ball's 44 56\n",
      "50 6 57 58\n",
      "50 Lakers 88 94\n",
      "54 RHONJ 15 20\n",
      "54 Teresa 70 76\n",
      "58 UK 117 119\n",
      "58 US 146 148\n",
      "58 Phoebe Tonkins 173 187\n",
      "58 The Secret Circle 194 211\n",
      "59 200 29 32\n",
      "60 Brees 50 55\n",
      "61 two 263 266\n",
      "62 2 hours 0 7\n",
      "63 Heatwaffle 0 10\n",
      "63 280 11 14\n",
      "64 Puerto rico 81 92\n",
      "65 next week 47 56\n",
      "65 Brees 83 88\n",
      "65 next week 104 113\n",
      "69 one 235 238\n",
      "69 our precious days 242 259\n",
      "69 first 389 394\n",
      "70 At least 1 0 10\n",
      "71 Arpaio 140 146\n",
      "76 Te KƒÅ 33 38\n",
      "76 Muana 44 49\n",
      "80 some 4 cent 11 22\n",
      "83 143413728| &gt 0 14\n",
      "84 Aprils Fool 118 129\n",
      "84 Wiz 151 154\n",
      "84 first 334 339\n",
      "87 Islam 199 204\n",
      "87 Islam 509 514\n",
      "87 Islam 648 653\n",
      "87 Muslims 658 665\n",
      "87 years 905 910\n",
      "87 those years 971 982\n",
      "88 Gutenberg 46 55\n",
      "88 Gutenberg 141 150\n",
      "92 Doggo 105 110\n",
      "92 33s 128 131\n",
      "92 Precision 212 221\n",
      "92 CC 332 334\n",
      "95 LosAngeles!](https://reddit.com/r/RandomActsOfBlowJob 59 112\n",
      "95 RSS 279 282\n",
      "97 1 188 189\n",
      "97 5 231 232\n",
      "97 1 281 282\n",
      "97 655 348 351\n",
      "97 11 374 376\n",
      "97 655 378 381\n",
      "97 765 383 386\n",
      "99 Ossoff 55 61\n",
      "99 earlier in the year 101 120\n",
      "101 Leonardo da Vinci 6 23\n",
      "101 Mona Lisa' 44 54\n",
      "101 Rule 34 90 97\n",
      "103 Crimson Hexphase 0 16\n",
      "109 One 0 3\n",
      "112 Journey 122 129\n",
      "112 Ixalan 176 182\n",
      "117 7 39 40\n",
      "117 first 168 173\n",
      "117 7 285 286\n",
      "117 /battleground 353 366\n",
      "117 one 582 585\n",
      "117 4-5 years 622 631\n",
      "117 13 668 670\n",
      "119 2 0 1\n",
      "120 1100 68 72\n",
      "120 one 99 102\n",
      "123 Clemson 21 28\n",
      "123 tonight 29 36\n",
      "124 like 2 years 88 100\n",
      "126 one 0 3\n",
      "128 FSU 42 45\n",
      "128 a 1st 122 127\n",
      "128 FSU 141 144\n",
      "129 only two 21 29\n",
      "129 Cordelia 30 38\n",
      "129 IV 186 188\n",
      "130 Lake St. 35 43\n",
      "130 Italian 66 73\n",
      "131 Rockstar 23 31\n",
      "131 one 58 61\n",
      "131 Irish 164 169\n",
      "131 Arthur 350 356\n",
      "132 a few months ago 38 54\n",
      "135 MCU 30 33\n",
      "135 Avengers 1 63 73\n",
      "139 CTE 33 36\n",
      "140 Pearl 25 30\n",
      "140 yesterday 81 90\n",
      "140 summer 206 212\n",
      "141 1 81 82\n",
      "143 one 0 3\n",
      "143 japanese 20 28\n",
      "146 143413837| &gt 0 14\n",
      "146 France Anonymous 16 32\n",
      "146 2008 103 107\n",
      "148 FYI 0 3\n",
      "148 the Caddy Limo 5 19\n",
      "148 first 40 45\n",
      "148 yesterday 92 101\n",
      "151 Tabantha Wheat + 26 42\n",
      "155 Gangs and Start 92 107\n",
      "156 VT 3 5\n",
      "156 GOBBLE 20 26\n",
      "156 this week 55 64\n",
      "156 Clemson 73 80\n",
      "157 Voyager 19 26\n",
      "158 Clemson 3 10\n",
      "159 Espn 5 9\n",
      "161 2 102 103\n",
      "161 3 129 130\n",
      "163 Nty 0 3\n",
      "164 SuperDisk 0 9\n",
      "167 12 11 13\n",
      "167 first 21 26\n",
      "171 4 31 32\n",
      "172 0.2-0.3 43 50\n",
      "172 2 87 88\n",
      "172 30sec+ 98 104\n",
      "172 one 141 144\n",
      "172 2 150 151\n",
      "172 about half 164 174\n",
      "174 TVTropes 30 38\n",
      "178 One 0 3\n",
      "178 100% 23 27\n",
      "183 our good years 74 88\n",
      "183 Les 99 102\n",
      "183 Hatter 119 125\n",
      "183 14 173 175\n",
      "185 2 85 86\n",
      "186 K/D 0 3\n",
      "187 two 48 51\n",
      "187 two 86 89\n",
      "187 10 160 162\n",
      "187 20th 199 203\n",
      "192 one 40 43\n",
      "193 Niki Ashton 4 15\n",
      "193 Hillary Clinton 83 98\n",
      "198 Hajimete no Gal 0 15\n",
      "201 SQM 410 413\n",
      "201 the Hunt Analyzer 945 962\n",
      "201 Preys 964 969\n",
      "202 4.1 51 54\n",
      "202 Tennessee 107 116\n",
      "205 One 0 3\n",
      "207 today 19 24\n",
      "208 One 1 4\n",
      "209 ^you 76 80\n",
      "210 Tamir Rice 20 30\n",
      "211 PayPal 16 22\n",
      "213 millions of dollars 61 80\n",
      "213 Puerto Ricos 195 207\n",
      "213 Minecraft 298 307\n",
      "213 the Puerto Ricans 380 397\n",
      "213 Puerto Rico 467 478\n",
      "213 the past 24 hours 484 501\n",
      "213 Puerto Rico 562 573\n",
      "213 the past week 592 605\n",
      "213 CNN 633 636\n",
      "213 Tuesday 713 720\n",
      "213 Saturday 841 849\n",
      "213 the entire week 860 875\n",
      "213 US 909 911\n",
      "213 American 917 925\n",
      "213 an hour 951 958\n",
      "215 Mickey Gall 0 11\n",
      "215 only 1 16 22\n",
      "217 2 secs 112 118\n",
      "218 Neyland 11 18\n",
      "218 this year 19 28\n",
      "220 Psamathe 72 80\n",
      "220 Orion 86 91\n",
      "221 ü§¢ü§¢ü§¢ 0 3\n",
      "222 Turing 2 8\n",
      "223 6 4 5\n",
      "223 a few minutes 38 51\n",
      "223 Moon 105 109\n",
      "224 Briscoe 9 16\n",
      "226 North Moorhead 16 30\n",
      "226 2-3 years ago 31 44\n",
      "226 15th 65 69\n",
      "226 11th 92 96\n",
      "226 11th 138 142\n",
      "226 Fargo 185 190\n",
      "226 every year 255 265\n",
      "226 19th 284 288\n",
      "226 fourth 303 309\n",
      "226 the last five years 318 337\n",
      "227 Christian 17 26\n",
      "228 Reagan 45 51\n",
      "228 Supreme 53 60\n",
      "229 143413069| &gt 0 14\n",
      "229 Canada Anonymous 16 32\n",
      "229 8h4XA49X 38 46\n",
      "230 McCain 27 33\n",
      "232 299 15 18\n",
      "233 9.99 14 18\n",
      "235 one 3 6\n",
      "235 only 1 225 231\n",
      "236 Danny Mills 0 11\n",
      "243 Miralo 0 6\n",
      "245 200 37 40\n",
      "245 Balance&amp;message 397 416\n",
      "245 Donate&amp;message 579 597\n",
      "246 F√ºnf 12 16\n",
      "250 more like $10 million dollars 7 36\n",
      "250 the Charter of Rights and Freedoms 98 132\n",
      "250 Canadian 148 156\n",
      "250 Trudeau 172 179\n",
      "250 Khadr 185 190\n",
      "250 $10 million 234 245\n",
      "250 Khadr 316 321\n",
      "250 $10 million 330 341\n",
      "250 Canadian 447 455\n",
      "251 Touche 0 6\n",
      "254 Pepe 8 12\n",
      "255 FEMA 31 35\n",
      "255 Trump 159 164\n",
      "255 Yulin Cruz 227 237\n",
      "255 FEMA 331 335\n",
      "255 anti-USA 378 386\n",
      "255 Damnit 434 440\n",
      "261 Havr 10 14\n",
      "262 50 percent 8 18\n",
      "262 Roman 56 61\n",
      "262 25 minutes 161 171\n",
      "262 two 259 262\n",
      "262 Cena 412 416\n",
      "262 Roman 467 472\n",
      "262 Cena Vs Michaels 497 513\n",
      "262 Cena Vs Rollins 515 530\n",
      "262 Cena Vs Punk 574 586\n",
      "262 three 600 605\n",
      "262 Rollins 615 622\n",
      "262 Lesnar 627 633\n",
      "262 two 724 727\n",
      "263 NA 145 147\n",
      "263 SA 151 153\n",
      "263 EU 157 159\n",
      "263 ASIA 163 167\n",
      "263 EU 321 323\n",
      "263 California 363 373\n",
      "263 5th 392 395\n",
      "263 ASIA 407 411\n",
      "265 8 weeks ago 28 39\n",
      "265 5k 143 145\n",
      "266 JediMasterBen 10 23\n",
      "266 TOTM 80 84\n",
      "266 Reef2Reef 88 97\n",
      "266 Coralvue 407 415\n",
      "267 USA 9 12\n",
      "267 HOA 29 32\n",
      "273 two 26 29\n",
      "273 Konahrik 46 54\n",
      "274 first 30 35\n",
      "275 Zai 0 3\n",
      "280 the last minute 131 146\n",
      "282 Oct. 1st 83 91\n",
      "282 Sunday 131 137\n",
      "282 Thursday 207 215\n",
      "282 the same day 278 290\n",
      "282 Wednesday 318 327\n",
      "282 Wednesday 361 370\n",
      "282 Thursday 375 383\n",
      "282 Wednesday 388 397\n",
      "282 tomorrow 448 456\n",
      "282 Wednesday 461 470\n",
      "283 neo-Nazis 81 90\n",
      "283 neo-Nazis 157 166\n",
      "283 neo-Nazis 190 199\n",
      "283 neo-Nazis 232 241\n",
      "283 Nazis 269 274\n",
      "287 thousands of years 53 71\n",
      "287 today 130 135\n",
      "296 7 88 89\n",
      "296 https://www.reddit.com/r/CrimeScene/comments/6ryulq/soldiers_pov_during_operation_overlord_dday_june/ % 467 570\n",
      "298 zero 71 75\n",
      "300 Baka Tsuki 27 37\n",
      "303 Texas 21 26\n",
      "305 EDA 24 27\n",
      "305 12 hours 85 93\n",
      "305 DDA 156 159\n",
      "305 The Bitcoin Cash 317 333\n",
      "305 DAA 363 366\n",
      "305 Bitcoin Core 429 441\n",
      "305 DAA 458 461\n",
      "307 this season 22 33\n",
      "307 CP3  \n",
      "SG 62 70\n",
      "307 Gordon  \n",
      " 72 81\n",
      "307 Melo 100 104\n",
      "308 English 219 226\n",
      "308 English 313 320\n",
      "310 ‚Äôm 75 77\n",
      "312 143414817| &gt 0 14\n",
      "312 United States Anonymous 16 39\n",
      "313 1d20 0 4\n",
      "313 Razqn 8 13\n",
      "313 1 37 38\n",
      "313 1 44 45\n",
      "313 13 83 85\n",
      "313 13 90 92\n",
      "318 yesterday 125 134\n",
      "318 Bodak Yellow 205 217\n",
      "318 two 251 254\n",
      "319 18 133 135\n",
      "322 first 283 288\n",
      "326 Podesta 41 48\n",
      "327 absolutely 100% 131 146\n",
      "327 OP 223 225\n",
      "332 Greinke 0 7\n",
      "335 Stalin 125 131\n",
      "335 Trofim Lysenko 134 148\n",
      "335 Soviet 197 203\n",
      "336 Haha thanks!Very 0 16\n",
      "336 mm 227 229\n",
      "336 5 300 301\n",
      "336 one day 573 580\n",
      "336 the season ended 588 604\n",
      "336 600 652 655\n",
      "336 the last month of the season 959 987\n",
      "336 200 1003 1006\n",
      "336 600 1010 1013\n",
      "336 one month 1017 1026\n",
      "336 200 1249 1252\n",
      "336 5 1273 1274\n",
      "336 One 1354 1357\n",
      "336 3 1365 1366\n",
      "336 first 1499 1504\n",
      "336 30 1591 1593\n",
      "336 3 1690 1691\n",
      "336 5 1728 1729\n",
      "336 5 1791 1792\n",
      "336 the wee hours of the morning 2168 2196\n",
      "336 600 2332 2335\n",
      "336 400 2427 2430\n",
      "336 7am 2462 2465\n",
      "336 the next season 2584 2599\n",
      "336 600 2695 2698\n",
      "337 ESPN3 25 30\n",
      "339 Bud Light 0 9\n",
      "339 Bud Light 88 97\n",
      "342 the Enemy Within 48 64\n",
      "342 Theowyn 68 75\n",
      "342 HPG 79 82\n",
      "342 Potter 113 119\n",
      "342 Theowyn 145 152\n",
      "344 tonight 23 30\n",
      "348 the day 301 308\n",
      "348 one 345 348\n",
      "348 the week 844 852\n",
      "348 the weekends 1045 1057\n",
      "349 3 27 28\n",
      "351 2 7 8\n",
      "351 one 45 48\n",
      "352 Valve 21 26\n",
      "352 over 100 52 60\n",
      "352 Overwatch 80 89\n",
      "353 Yeh 0 3\n",
      "356 Dankquan 29 37\n",
      "362 Henrich 24 31\n",
      "362 6 50 51\n",
      "362 two 64 67\n",
      "365 Beardownüêª‚¨áÔ∏è 51 62\n",
      "366 two 84 87\n",
      "366 First 112 117\n",
      "366 Second 222 228\n",
      "366 Purple 310 316\n",
      "366 one 526 529\n",
      "366 Aloe & 543 549\n",
      "366 Lotus 554 559\n",
      "367 Gatsby 13 19\n",
      "367 Chad 30 34\n",
      "367 Lake 132 136\n",
      "367 Daisy Buchanan] 228 243\n",
      "367 Daisy 347 352\n",
      "367 Gatsby 392 398\n",
      "369 Chad 49 53\n",
      "369 Chad 70 74\n",
      "369 Chad 154 158\n",
      "370 2933 53 57\n",
      "370 2133 78 82\n",
      "371 Weber 16 21\n",
      "371 McCormick 31 40\n",
      "374 Halatali 92 100\n",
      "375 Audrey 5 11\n",
      "375 Double 70 76\n",
      "375 Audrey 87 93\n",
      "378 Pumas 45 50\n",
      "384 Patrick 142 149\n",
      "384 Ryan 166 170\n",
      "384 the season 314 324\n",
      "386 United States Anonymous 16 39\n",
      "387 Alarak 117 123\n",
      "387 Sonya 189 194\n",
      "387 Gazlowe 215 222\n",
      "389 Freeza 103 109\n",
      "397 Patrick Swayze 27 41\n",
      "401 a ton 17 22\n",
      "401 the day 246 253\n",
      "401 these days 603 613\n",
      "402 100% 8 12\n",
      "402 Susperia 47 55\n",
      "402 100 79 82\n",
      "404 Vecna 64 69\n",
      "404 this week 82 91\n",
      "406 Don Lemon 42 51\n",
      "406 Lemon 62 67\n",
      "406 Cernavich 87 96\n",
      "408 Turkey 39 45\n",
      "408 Thanksgiving 53 65\n",
      "413 Twitter 377 384\n",
      "420 55 167 169\n",
      "420 26 208 210\n",
      "420 5 306 307\n",
      "424 30-30 18 23\n",
      "425 DIY 253 256\n",
      "425 TKB Trading 281 292\n",
      "426 monday 74 80\n",
      "431 Dan 166 169\n",
      "431 YouTube 261 268\n",
      "433 IM 39 41\n",
      "433 K 50 51\n",
      "434 Simpsons 4 12\n",
      "436 Jon 65 68\n",
      "436 Cersei 91 97\n",
      "437 Apple 52 57\n",
      "438 QB 17 19\n",
      "438 January of this year 32 52\n",
      "439 Star Wars 33 42\n",
      "440 92 25 27\n",
      "440 92 129 131\n",
      "440 92 260 262\n",
      "441 second 53 59\n",
      "441 Duplass Brothers 244 260\n",
      "442 Gordon 0 6\n",
      "442 60 70 72\n",
      "447 the day 107 114\n",
      "448 Anonymous 21 30\n",
      "448 K4S0eFk8 36 44\n",
      "449 01 12 14\n",
      "449 Jason Ross 16 26\n",
      "449 Lauren Ray ‚Äì I 33 47\n",
      "449 Seven 79 84\n",
      "449 Lions & 85 92\n",
      "449 Xilent 97 103\n",
      "449 Jason Ross ‚Äì Cairo 124 142\n",
      "449 Seven 160 165\n",
      "449 KARRA ‚Äì Silent Skies 178 198\n",
      "449 03 214 216\n",
      "449 Seven 218 223\n",
      "449 Lions ‚Äì Steps Of Deep Slumber 224 253\n",
      "449 Jason Ross 272 282\n",
      "449 Lauren Ray 289 299\n",
      "449 04 318 320\n",
      "449 Wrechiski &amp 322 336\n",
      "449 Jason Ross ‚Äì Atlas 338 356\n",
      "449 05 371 373\n",
      "449 Seven 375 380\n",
      "449 Lions ‚Äì Cusp 381 393\n",
      "449 06 416 418\n",
      "449 Seven 420 425\n",
      "449 Lions 426 431\n",
      "449 Skyler Stonestreet ‚Äì Freesol 438 466\n",
      "449 07 482 484\n",
      "449 Jason Ross ‚Äì Valor 486 504\n",
      "449 Seven 506 511\n",
      "449 Jason Ross ‚Äì Mirror Image 542 567\n",
      "449 09 582 584\n",
      "449 Seven 586 591\n",
      "449 Lions & 592 599\n",
      "449 Jason Ross 604 614\n",
      "449 Jonathan Mendelsohn 621 640\n",
      "449 10 649 651\n",
      "449 Seven 677 682\n",
      "449 Lions & 683 690\n",
      "449 Dimibo Remix 695 707\n",
      "449 11 718 720\n",
      "449 Seven 722 727\n",
      "449 Lions & 728 735\n",
      "449 Jason Ross ‚Äì ID 740 755\n",
      "449 12 756 758\n",
      "449 Seven 760 765\n",
      "449 Lions 766 771\n",
      "449 Ellie Goulding 778 792\n",
      "449 13 820 822\n",
      "449 Jason Ross ‚Äì Coaster [ 824 846\n",
      "449 Ilan Bluestone & 862 878\n",
      "449 Jason Ross ‚Äì Amun 883 900\n",
      "449 ANJUNABEATS 902 913\n",
      "449 Seven 918 923\n",
      "449 14 928 930\n",
      "449 Seven 932 937\n",
      "449 Lions 938 943\n",
      "449 V√∂k ‚Äì Creation 950 964\n",
      "449 15 978 980\n",
      "449 Seven 982 987\n",
      "449 Lions 988 993\n",
      "449 Lights ‚Äì Falling Away 1000 1021\n",
      "449 16 1050 1052\n",
      "449 Seven 1054 1059\n",
      "449 Lions & 1060 1067\n",
      "449 Jason Ross 1072 1082\n",
      "449 Paul Meany 1089 1099\n",
      "449 17 1128 1130\n",
      "449 Dirty South 1132 1143\n",
      "449 ANIMA 1150 1155\n",
      "449 Jason Ross Remix 1168 1184\n",
      "449 Seven 1203 1208\n",
      "449 Lions & 1209 1216\n",
      "449 Echos ‚Äì Cold S 1221 1235\n",
      "449 18 1236 1238\n",
      "449 Seven 1240 1245\n",
      "449 Lions 1246 1251\n",
      "449 Kerli ‚Äì Worlds Apart 1258 1278\n",
      "449 ABGT250 Outro Edit 1280 1298\n",
      "452 Vivid Seats 22 33\n",
      "452 30 152 154\n",
      "454 China 45 50\n",
      "458 Hangin 31 37\n",
      "458 Cooper 47 53\n",
      "459 Houston Chronicle 0 17\n",
      "459 a few hours 167 178\n",
      "465 Two 58 61\n",
      "465 Elise 244 249\n",
      "465 2 292 293\n",
      "465 2 329 330\n",
      "469 50k 216 219\n",
      "470 winter 73 79\n",
      "471 about 25lbs 92 103\n",
      "473 my tenth month 7 21\n",
      "473 two months 64 74\n",
      "475 0% 11 13\n",
      "475 WV 40 42\n",
      "475 week 1 next year 43 59\n",
      "475 the next 2 days 104 119\n",
      "475 Brady Hoke 134 144\n",
      "477 Puerto Rico 15 26\n",
      "477 Uncle Sam 74 83\n",
      "479 one 52 55\n",
      "480 THICC 0 5\n",
      "485 1:100000000 71 82\n",
      "488 OP 41 43\n",
      "488 first 46 51\n",
      "488 3h 192 194\n",
      "488 3h 254 256\n",
      "488 1h 269 271\n",
      "488 2 316 317\n",
      "488 Korea 384 389\n",
      "488 BBQ Olivers 415 426\n",
      "488 Afreeca Freecs 432 446\n",
      "488 NA 535 537\n",
      "489 MTV 38 41\n",
      "489 200 65 68\n",
      "489 Instagram 83 92\n",
      "492 Estie c'est profond 0 19\n",
      "494 Foids 17 22\n",
      "494 her late 30s 373 385\n",
      "494 india 578 583\n",
      "495 Qari 39 43\n",
      "495 X. 110 112\n",
      "498 Brb 0 3\n",
      "499 FUCKSTICKS 372 382\n",
      "500 Simpsons 51 59\n",
      "501 Garza 0 5\n",
      "502 Spain Pool\"\n",
      "\n",
      "&amp;nbsp 34 56\n",
      "502 ImagesOfNetwork 168 183\n",
      "502 Questions)](https://www.reddit.com 230 264\n",
      "502 post!\")](https://www.reddit.com 433 464\n",
      "502 wrong!\")](https://www.reddit.com 556 588\n",
      "502 Spain Pool 853 863\n",
      "504 143417408| &gt 0 14\n",
      "504 Switzerland Anonymous 16 37\n",
      "505 Universe 109 117\n",
      "505 Universe 179 187\n",
      "506 ten 151 154\n",
      "509 CPU 38 41\n",
      "509 Intel 43 48\n",
      "509 Ryzen 70 75\n",
      "509 5 1600 76 82\n",
      "510 760 9 12\n",
      "510 60$ 17 20\n",
      "511 one night 200 209\n",
      "512 Red Dwarf 25 34\n",
      "513 50 28 30\n",
      "514 OP 26 28\n",
      "515 Butthurt 0 8\n",
      "515 China 9 14\n",
      "516 Rams 33 37\n",
      "520 95 seconds 86 96\n",
      "520 15 seconds 112 122\n",
      "520 ^^| ^^/r/ImageStabilization/ ^^| ^^for ^^cropped ^^results 356 414\n",
      "522 UK 88 90\n",
      "524 Driller Killer 9 23\n",
      "524 Laser Mission 170 183\n",
      "524 PUNCH 232 237\n",
      "524 One 240 243\n",
      "524 Rob 'Alpacapatrol' Robert. 324 350\n",
      "529 Chelsea 0 7\n",
      "529 Michael Cera 22 34\n",
      "529 Michael Cera 93 105\n",
      "530 Space Gray 56 66\n",
      "531 one 12 15\n",
      "531 6 hours ago 58 69\n",
      "532 7.8/10 15 21\n",
      "533 two 60 63\n",
      "534 Pok√©mon 81 88\n",
      "537 Tina 10 14\n",
      "537 Gtfo 26 30\n",
      "539 Heinz 0 5\n",
      "539 1 47 48\n",
      "539 2 53 54\n",
      "541 conservatives 5 18\n",
      "541 these days 46 56\n",
      "542 Lee 5 8\n",
      "543 PER 56 59\n",
      "543 Utah 124 128\n",
      "543 2 259 260\n",
      "543 1000 305 309\n",
      "546 4 25 26\n",
      "546 LR Goku 30 37\n",
      "548 Winnie the Pooh 32 47\n",
      "549 Hail Mary's 0 11\n",
      "550 Oliver 36 42\n",
      "550 Laurel 47 53\n",
      "554 millennia 167 176\n",
      "554 Spanish 415 422\n",
      "554 Bullfighting Bulls 423 441\n",
      "554 Yay 534 537\n",
      "557 a few weeks 22 33\n",
      "557 Trump 44 49\n",
      "557 the United States 62 79\n",
      "557 Puerto Rico 100 111\n",
      "557 ALPHA 174 179\n",
      "559 a nice day 59 69\n",
      "561 7th 50 53\n",
      "562 Americans 42 51\n",
      "562 3 107 108\n",
      "562 Section 8 150 159\n",
      "564 Cell 54 58\n",
      "564 Ganos 90 95\n",
      "564 Ganos 137 142\n",
      "564 Roshi 222 227\n",
      "565 3.9 107 110\n",
      "565 GPA 111 114\n",
      "565 CC's(Community College's 165 189\n",
      "565 CC Courses 394 404\n",
      "565 University 515 525\n",
      "565 GI 547 549\n",
      "565 FAFSA 573 578\n",
      "565 University 677 687\n",
      "565 MCWIS 711 716\n",
      "567 AFPC 24 28\n",
      "567 IG 115 117\n",
      "568 143413367| &gt 0 14\n",
      "568 United States Anonymous 16 39\n",
      "569 Chicago 39 46\n",
      "570 6 4 5\n",
      "570 6th 54 57\n",
      "572 Lee 26 29\n",
      "573 SCR 36 39\n",
      "574 80 17 19\n",
      "574 100 37 40\n",
      "574 night 61 66\n",
      "574 two 100 103\n",
      "574 at least 10 degrees 147 166\n",
      "574 Texas 336 341\n",
      "574 DFW 372 375\n",
      "577 Watergate 11 20\n",
      "577 Nixon 22 27\n",
      "579 KD 43 45\n",
      "580 DELET 0 5\n",
      "581 Late last year 9 23\n",
      "581 Japan 49 54\n",
      "581 Japan 97 102\n",
      "581 a few weeks every year 107 129\n",
      "581 38 year old 168 179\n",
      "581 Hiroko 192 198\n",
      "581 Kyoto 220 225\n",
      "581 Japan 348 353\n",
      "581 English 581 588\n",
      "581 a few years ago 787 802\n",
      "581 Line 1009 1013\n",
      "581 Osaka 1112 1117\n",
      "581 the next day 1118 1130\n",
      "581 a couple days 1149 1162\n",
      "581 Line 1166 1170\n",
      "581 the weekend 1246 1257\n",
      "581 Kyoto 1352 1357\n",
      "581 Airbnb 1478 1484\n",
      "581 Osaka Castle 1524 1536\n",
      "581 a few minutes later 2421 2440\n",
      "581 Japanese 2470 2478\n",
      "581 a couple of hours 2931 2948\n",
      "581 Japanese 4442 4450\n",
      "581 a couple of minutes 4875 4894\n",
      "581 night 5596 5601\n",
      "581 the next morning 5636 5652\n",
      "581 Japan 5882 5887\n",
      "584 the day 32 39\n",
      "587 two 51 54\n",
      "588 season 6 116 124\n",
      "590 Angels | Home | 37 52\n",
      "590 English 53 60\n",
      "590 1 66 67\n",
      "590 Yes](http://sportshd.me/mlb/492512/h 80 116\n",
      "590 English 157 164\n",
      "590 1 170 171\n",
      "591 two 148 151\n",
      "591 30 years 490 498\n",
      "591 the day 721 728\n",
      "592 Spain Pool\"\n",
      "\n",
      "&amp;nbsp 34 56\n",
      "592 ImagesOfNetwork 168 183\n",
      "592 Questions)](https://www.reddit.com 230 264\n",
      "592 post!\")](https://www.reddit.com 433 464\n",
      "592 wrong!\")](https://www.reddit.com 556 588\n",
      "592 Spain Pool 853 863\n",
      "594 25 8 10\n",
      "595 CE 4 6\n",
      "595 Shirou, Rin, 10 22\n",
      "595 Sakura 27 33\n",
      "596 Solo 18 22\n",
      "599 2 1086 1087\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:600]):\n",
    "    for e in doc.ents:\n",
    "        print(i, e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, let's not forget that, as in any machine learning model, we are also prone to errors in our prediction.\n",
    "\n",
    "Could we train a better model? Sure! Given a good corpus for training and the right tools we could achieve a very high accuracy. However, as this is not the objective of this BLU we are going to leave you some links if you want to learn more about this.\n",
    "\n",
    "https://spacy.io/usage/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another handy link - https://spacy.io/usage/linguistic-features - you can find here all kind of features SpaCy can extract for you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
