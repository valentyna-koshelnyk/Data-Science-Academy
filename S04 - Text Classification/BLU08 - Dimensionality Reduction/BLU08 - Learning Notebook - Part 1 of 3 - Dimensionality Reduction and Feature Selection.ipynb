{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Dimensionality Reduction and Feature Selection\n",
    "\n",
    "\n",
    "Thinking in high dimensions is particularly hard. A 1-D dot would hardly be able to imagine a 2D world. In the same way, the square below struggles to think of our 3D world. For us, it is very difficult to imagine dimensions above the 3rd. We have proxies for thinking of a 4th dimension - passing of time, a 3D surface with some extra measure represented by color, etc - but this starts to become really difficult to work above the 4th dimension.  \n",
    "\n",
    "![dimensionality](./media/flatland.png)\n",
    "\n",
    "Thankfully, when dealing with ML problems, you can go above three features without trying to imagine how they would look like (Lucky you!). However, you still need to understand how the number of dimensions might affect - either positively or negatively - your models and how to work with high dimensional spaces. Don't worry, we will guide you through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import barh,plot,yticks,show,grid,xlabel,figure,cla,close\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The curse of dimensionality\n",
    "\n",
    "So far you learned how to handle text data by transforming it into a vectorized feature space. Namely, we mostly used preprocessing tricks and some sort of count over words - remember the CountVectorizer and TF-IDF - to generate our feature space. However, as you might have realized, the number of features for these problems is huge, in particular if you want to cover all language. In the limit, your feature space will cover the entire vocabulary!\n",
    "\n",
    "\n",
    "The effect of high dimensionality features when modelling these problems is called the **curse of dimensionality**. You can probably already see some of the effects of these curse, right?\n",
    "\n",
    "* To start with, more features, will obviously mean a longer training process\n",
    "* On another hand, a higher dimensionality can actually hurt the classifier performance\n",
    "\n",
    "\n",
    "\n",
    "#### But I thought \"more features\" meant \"more accuracy\"...\n",
    "\n",
    "This is not always true. When you have high dimensionality problems, the use of all the features might actually hurt your model. The model becomes more prone to overfitting and might have worse accuracy in points outside your training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.  Basic Feature Selection\n",
    "\n",
    "One of the ways you might think of to reduce your feature dimensionality is by performing feature selection. We are going to walk you through some methods with an actual example. Let's start by loading some data - we are going to use the twitter dataset of republican and democrat tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Get your dataset\n",
    "\n",
    "Start by importing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/twitter_rep_dem_data_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Get to know our problem \n",
    "\n",
    "We'll first learn our categories and see a few examples of how our training data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:\n",
      "Democrat, Republican\n"
     ]
    }
   ],
   "source": [
    "print('Categories:')\n",
    "print(', '.join(set(df.Party)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check class balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Republican    9302\n",
       "Democrat      9217\n",
       "Name: Party, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Party.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty balanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Today, Senate Dems vote to #SaveTheInternet. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @WinterHavenSun: Winter Haven resident / Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NBCLatino: .@RepDarrenSoto noted that Hurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @NALCABPolicy: Meeting with @RepDarrenSoto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @Vegalteno: Hurricane season starts on June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @EmgageActionFL: Thank you to all who came ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Hurricane Maria left approx $90 billion in dam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @Tharryry: I am delighted that @RepDarrenSo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @HispanicCaucus: Trump's anti-immigrant pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>RT @RepStephMurphy: Great joining @WeAreUnidos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Party         Handle                                              Tweet\n",
       "0  Democrat  RepDarrenSoto  Today, Senate Dems vote to #SaveTheInternet. P...\n",
       "1  Democrat  RepDarrenSoto  RT @WinterHavenSun: Winter Haven resident / Al...\n",
       "2  Democrat  RepDarrenSoto  RT @NBCLatino: .@RepDarrenSoto noted that Hurr...\n",
       "3  Democrat  RepDarrenSoto  RT @NALCABPolicy: Meeting with @RepDarrenSoto ...\n",
       "4  Democrat  RepDarrenSoto  RT @Vegalteno: Hurricane season starts on June...\n",
       "5  Democrat  RepDarrenSoto  RT @EmgageActionFL: Thank you to all who came ...\n",
       "6  Democrat  RepDarrenSoto  Hurricane Maria left approx $90 billion in dam...\n",
       "7  Democrat  RepDarrenSoto  RT @Tharryry: I am delighted that @RepDarrenSo...\n",
       "8  Democrat  RepDarrenSoto  RT @HispanicCaucus: Trump's anti-immigrant pol...\n",
       "9  Democrat  RepDarrenSoto  RT @RepStephMurphy: Great joining @WeAreUnidos..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that our tweets are just text with some particularities. For example, it is common to have twitter handles in the text, defined by the \"@\" character. Our data also has three columns, but we are going to ignore the *Handle* column for now and just focus on classifying *Tweets* with *Party* labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Feature Extraction\n",
    "\n",
    "Our data is simply raw text, each element a document to be classified, with a corresponding label, which is the Party of the tweet. Pretty simple, right?\n",
    "\n",
    "Since you are a great student, you went thoroughly through BLU07 and you already know how to prepare, handle text and extract some simple features. So let's process our data and use TfidfVectorizer with a range of 1-2 ngrams to get us some simple features.\n",
    "\n",
    "First let's apply simple tokenization and remove punctuation. To avoid overfitting to twitter related information, like the handles tagged in the messages, let's remove those, so our focus is only on the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Handle</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>today , senate dems vote to # savetheinternet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : winter haven resident / alta vista teache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : . noted that hurricane maria has left app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : meeting with . thanks for taking the time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : hurricane season starts on june 1st ; pue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : thank you to all who came out to our orla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>hurricane maria left approx $ 90 billion in da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : i am delighted that will be voting for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : trump ' s anti - immigrant policies are h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>rt : great joining and for a roundtable in # o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Party         Handle                                              Tweet\n",
       "0  Democrat  RepDarrenSoto  today , senate dems vote to # savetheinternet ...\n",
       "1  Democrat  RepDarrenSoto  rt : winter haven resident / alta vista teache...\n",
       "2  Democrat  RepDarrenSoto  rt : . noted that hurricane maria has left app...\n",
       "3  Democrat  RepDarrenSoto  rt : meeting with . thanks for taking the time...\n",
       "4  Democrat  RepDarrenSoto  rt : hurricane season starts on june 1st ; pue...\n",
       "5  Democrat  RepDarrenSoto  rt : thank you to all who came out to our orla...\n",
       "6  Democrat  RepDarrenSoto  hurricane maria left approx $ 90 billion in da...\n",
       "7  Democrat  RepDarrenSoto  rt : i am delighted that will be voting for th...\n",
       "8  Democrat  RepDarrenSoto  rt : trump ' s anti - immigrant policies are h...\n",
       "9  Democrat  RepDarrenSoto  rt : great joining and for a roundtable in # o..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split our data and apply some vectorization. Let's pick a random seed so the results are replicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/random.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 38889\n",
      "Test examples: 16668\n",
      "\n",
      "1.27 s ± 88.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=seed)\n",
    "\n",
    "print('Training examples: {}'.format(train_data.size))\n",
    "print('Test examples: {}\\n'.format(test_data.size))\n",
    "\n",
    "# important: only train the vectorizer on the training data\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "%timeit vectorizer.fit(train_data.Tweet)\n",
    "\n",
    "X_train = vectorizer.transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "y_train = train_data.Party\n",
    "y_test = test_data.Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Getting our baseline\n",
    "\n",
    "Let's get our baseline accuracy and measure the time it takes to fit a Naive Bayes Model, a model you should be familiar with and which in NLP comes hand in hand with the Bag Of Words representation (if you don't get the joke below, eventually you should go read about naive bayes).\n",
    "\n",
    "<img src=\"./media/frequentists_vs_bayesians_2x.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5556x138409 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 150003 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.1 ms ± 4.42 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7595392368610511\n"
     ]
    }
   ],
   "source": [
    "clf =  MultinomialNB()\n",
    "%timeit clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Feature selection\n",
    "\n",
    "Now that we have our baseline, let's start by looking into the number of features used in our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12963, 138409)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, and our classifier even trained pretty fast, but a 140K-dimensional space is obviously very difficult to interpret (think back to the fact that even 4D is difficult for us to grasp). Let's instead try to extract our K most important words. One way that you might think to do this is actually to just get the features corresponding to the most frequent terms. In fact, our TfidfVectorizer already has an option for that. Let's see the impact on our training speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 features\n",
      "----\n",
      "39.3 ms ± 7.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.5259179265658748\n",
      "\n",
      "Using 100 features\n",
      "----\n",
      "30.6 ms ± 953 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.5815334773218143\n",
      "\n",
      "Using 1000 features\n",
      "----\n",
      "30.8 ms ± 1.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.6637868970482361\n",
      "\n",
      "Using 5000 features\n",
      "----\n",
      "38.9 ms ± 8.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7177825773938085\n",
      "\n",
      "Using 10000 features\n",
      "----\n",
      "32.8 ms ± 3.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7341612670986322\n",
      "\n",
      "Using 50000 features\n",
      "----\n",
      "33.2 ms ± 1.88 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7543196544276458\n",
      "\n",
      "Using 100000 features\n",
      "----\n",
      "32.7 ms ± 357 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7543196544276458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [10, 100, 1000, 5000, 10000, 50000, 100000]:\n",
    "    print('Using {} features'.format(k))\n",
    "    print('----'.format(k))\n",
    "    \n",
    "    X_train = train_data.Tweet\n",
    "    X_test = test_data.Tweet\n",
    "    \n",
    "    vectorizer_truncated = TfidfVectorizer(ngram_range=(1,2), max_features=k)\n",
    "    vectorizer_truncated.fit(X_train)\n",
    "\n",
    "    X_train_truncated = vectorizer_truncated.transform(X_train)\n",
    "    X_test_truncated = vectorizer_truncated.transform(X_test)\n",
    "    \n",
    "    clf =  MultinomialNB()\n",
    "    %timeit clf.fit(X_train_truncated, y_train)\n",
    "    y_pred = clf.predict(X_test_truncated)\n",
    "    \n",
    "    print('Accuracy: {}\\n'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so no amazing effects. Let's look into the actual top K-features to see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "co\n",
      "for\n",
      "https\n",
      "https co\n",
      "in\n",
      "of\n",
      "rt\n",
      "the\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "K=10\n",
    "vectorizer_truncated = TfidfVectorizer(ngram_range=(1,2), max_features=K)\n",
    "vectorizer_truncated.fit(X_train)\n",
    "feature_names = vectorizer_truncated.get_feature_names()\n",
    "for f in feature_names:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the top 10 features are basically meaningless when thinking of our classes. In the next cell you will see that the counts of these words are balanced between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_punct = lambda s: re.sub(r'\\s+\\W*[a-z]?\\W*\\s+', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents that contain the word \"and\"\n",
      "----\n",
      "Democrat      2410\n",
      "Republican    2334\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"co\"\n",
      "----\n",
      "Democrat      5571\n",
      "Republican    5493\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"for\"\n",
      "----\n",
      "Republican    2307\n",
      "Democrat      2188\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"https\"\n",
      "----\n",
      "Democrat      4818\n",
      "Republican    4702\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"https co\"\n",
      "----\n",
      "Democrat      4775\n",
      "Republican    4616\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"in\"\n",
      "----\n",
      "Republican    5103\n",
      "Democrat      5101\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"of\"\n",
      "----\n",
      "Democrat      2294\n",
      "Republican    2235\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"rt\"\n",
      "----\n",
      "Republican    2755\n",
      "Democrat      2670\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"the\"\n",
      "----\n",
      "Democrat      4191\n",
      "Republican    4139\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"to\"\n",
      "----\n",
      "Democrat      4287\n",
      "Republican    4182\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in feature_names:\n",
    "    print('Documents that contain the word \"{}\"'.format(feature))\n",
    "    print('----')\n",
    "    docs = X_train.apply(sub_punct).str.lower().str.contains(feature)\n",
    "    print(str(y_train[docs].value_counts()) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: Notice that most of these words are normally considered stopwords. Try to exclude stopwords from the TfidfVectorizer and see what new top features you obtain. You will probably see that there will be some common words and some \"political discourse\" words, but not really democrat/republican specific. \n",
    "\n",
    "But let's move on to more meaningful approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Feature Selection through statistical analysis\n",
    "\n",
    "Basic feature selection methods might actually work sometimes, in particular if you pick a reasonable heuristic to decide on which features to choose. In our case, obviously, just picking the higher counts is not that good, since it does not provide any useful information on the labels. But you could imagine for example trying to pick as features words that appear only (or almost only) on one of our classes.\n",
    "\n",
    "Although this might seem a good idea, depending on your problem, you would probably not want to lose that much time thinking about heuristics, implementing them and comparing them. This is where statistical tests are useful. You don't have to reason on the features you are using, these tests use your data to provide insights on your features.\n",
    "\n",
    "##### Chi-squared test\n",
    "\n",
    "The chi-squared test is one of these tests. The chi-squared formula measures how much expected counts and observed counts of variables/distributions deviate from each other. It can be used to test for independce between two variables, like defined by the equation below, where $O_{x_1x_2}$ is the observation of the conjuction of variables and $E_{x_1x_2}$ the corresponding expected value, this is, the expected value given our hypothesis *$H_0$: the variables are independent*.\n",
    "\n",
    "$$\\chi^2 = \\sum{\\frac{(O_{x_1x_2} - E_{x_1x_2})^2}{E_{x_1x_2}}}$$\n",
    "\n",
    "For feature selection we want to test the independence of our features from the class labels. In our particular case, we define $x_1=t$ as our term or word and $x_2=c$ as our class label. A small chi-squared value will mean that the term is closer to independence from the class, and a big value that it is very dependent on the class.\n",
    "\n",
    "Knowing the details of the chi-squared can be useful for you, but in the context of our BLU is not our primary goal and there are more useful methods for text features that you will learn about in the following notebooks. However, we provide at the end of this BLU a more detailed explanation of this test, and some examples, if you wish to understand it better (see Annex A).\n",
    "\n",
    "We will lean on the previous example and show you how chi-squared would help us select more meaningful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Setup problem\n",
    "\n",
    "Like before, let's fetch our data, extract its features, run a baseline, and move from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985 ms ± 60.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "34.6 ms ± 899 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7595392368610511\n"
     ]
    }
   ],
   "source": [
    "stat_df = pd.read_csv('./datasets/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "stat_df['Tweet'] = stat_df['Tweet'].map(handle_removal)\n",
    "stat_df['Tweet'] = stat_df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "stat_train_data, stat_test_data = train_test_split(stat_df, test_size=0.3, random_state=seed)\n",
    "\n",
    "stat_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "%timeit stat_vectorizer.fit(stat_train_data.Tweet)\n",
    "\n",
    "stat_X_train = stat_vectorizer.transform(stat_train_data.Tweet)\n",
    "stat_X_test = stat_vectorizer.transform(stat_test_data.Tweet)\n",
    "\n",
    "stat_y_train = stat_train_data.Party\n",
    "stat_y_test = stat_test_data.Party\n",
    "\n",
    "stat_clf =  MultinomialNB()\n",
    "%timeit stat_clf.fit(stat_X_train, stat_y_train)\n",
    "\n",
    "stat_pred = stat_clf.predict(stat_X_test)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(stat_pred, stat_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Feature Selection from chi-squared\n",
    "\n",
    "We will now use the chi2 and obtain some chi-squared values for our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chi_values, p_values = chi2(stat_X_train, stat_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the most dependent features from the chi-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_names = stat_vectorizer.get_feature_names()\n",
    "\n",
    "cla()   # Clear axis\n",
    "close() # Close a figure window\n",
    "\n",
    "figure(figsize=(12,10))\n",
    "zipped_chi_squared = zip(feature_names, chi_values)\n",
    "sorted_chi_values = sorted(zipped_chi_squared, key=lambda x:x[1]) \n",
    "top_chi_values = list(zip(*sorted_chi_values[-30:]))\n",
    "\n",
    "x = range(len(top_chi_values[1]))\n",
    "labels = top_chi_values[0]\n",
    "barh(x, list(top_chi_values)[1], align='center', alpha=.2, color='g')\n",
    "yticks(x, labels)\n",
    "xlabel('$\\chi^2$')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, scikit-learn already provides a function to directly select the K-best features for our model, so we are going to use that to extract our most important features. We can confirm that the top features selected match the ones with the highest chi-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stat_X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-522461fd30c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mch2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mch2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmost_important_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mch2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_important_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stat_X_train' is not defined"
     ]
    }
   ],
   "source": [
    "ch2 = SelectKBest(chi2, k=10)\n",
    "ch2.fit(stat_X_train, stat_y_train)\n",
    "\n",
    "most_important_features = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "for f in most_important_features:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting somewhere, these new features are starting to make sense. We can look into their distribution of  over our documents to get a sense of the relation they share with the labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents that contain the word \"chairman\"\n",
      "----\n",
      "Republican    196\n",
      "Democrat        8\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"code\"\n",
      "----\n",
      "Republican    107\n",
      "Democrat        8\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"hearing\"\n",
      "----\n",
      "Republican    264\n",
      "Democrat       62\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"reform\"\n",
      "----\n",
      "Republican    426\n",
      "Democrat       34\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"tax\"\n",
      "----\n",
      "Republican    800\n",
      "Democrat      217\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"tax reform\"\n",
      "----\n",
      "Republican    126\n",
      "Democrat        6\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"taxcutsandjobsact\"\n",
      "----\n",
      "Republican    102\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"taxreform\"\n",
      "----\n",
      "Republican    227\n",
      "Democrat        2\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"texas\"\n",
      "----\n",
      "Republican    121\n",
      "Democrat        1\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contain the word \"the taxcutsandjobsact\"\n",
      "----\n",
      "Republican    86\n",
      "Name: Party, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in most_important_features:\n",
    "    print('Documents that contain the word \"{}\"'.format(feature))\n",
    "    print('----')\n",
    "    docs = stat_train_data['Tweet'].apply(sub_punct).str.contains(feature)\n",
    "    print(str(stat_y_train[docs].value_counts()) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some words are much more common in Republican tweets compared to Democrat tweets. These features are thus much more interpretable and might have a better impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 features\n",
      "----\n",
      "29.8 ms ± 648 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.4744420446364291\n",
      "\n",
      "Using 100 features\n",
      "----\n",
      "32 ms ± 8.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.5984521238300936\n",
      "\n",
      "Using 1000 features\n",
      "----\n",
      "32.9 ms ± 8.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.709143268538517\n",
      "\n",
      "Using 5000 features\n",
      "----\n",
      "30.2 ms ± 1.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7321814254859611\n",
      "\n",
      "Using 10000 features\n",
      "----\n",
      "30.4 ms ± 2.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7368610511159107\n",
      "\n",
      "Using 50000 features\n",
      "----\n",
      "31.3 ms ± 1.81 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7496400287976962\n",
      "\n",
      "Using 100000 features\n",
      "----\n",
      "31.9 ms ± 1.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7566594672426206\n",
      "\n",
      "Using all features\n",
      "----\n",
      "33.9 ms ± 1.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Accuracy: 0.7595392368610511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [10, 100, 1000, 5000, 10000, 50000, 100000, 'all']:\n",
    "    print('Using {} features'.format(k))\n",
    "    print('----'.format(k))\n",
    "    \n",
    "    ch2_train = SelectKBest(chi2, k=k)\n",
    "    ch2_train.fit(stat_X_train, stat_y_train)\n",
    "    X_train_chi = ch2_train.transform(stat_X_train)\n",
    "    X_test_chi = ch2_train.transform(stat_X_test)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    %timeit clf.fit(X_train_chi, stat_y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test_chi)\n",
    "\n",
    "    print('Accuracy: {}\\n'.format(accuracy_score(y_pred, stat_y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 100000 features we get only slightly better results. However, looking at the features themselves, we can clearly see this is a better feature selection method than the previous one, but we could still use a bit more gain and speed improvements.\n",
    "\n",
    "One thing that feature selection does not take into consideration is feature interaction, which can limit a bit the gains in performance. In the remaining of this BLU you will learn about more elaborate methods to perform dimensionality reduction, in particular some very recent methods that are the standard in text-related tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final remarks\n",
    "\n",
    "After reading this notebook you should understand:\n",
    "\n",
    " - What is the curse of dimensionality\n",
    " - How can you perform simple feature selection by reasoning about your problem\n",
    " - How to apply statistical methods for feature selection by finding dependencies between features and labels\n",
    " \n",
    "Keep in mind that predicting in the real world is much less theoretical. The performance of these methods will depend a lot on your problem, the size of your dataset, your model choice, your preprocessing. You can use feature selection to improve speed, avoid overfitting, or even just to interpret your features and how they interact with your classes. \n",
    "\n",
    "<br>\n",
    "\n",
    "-----\n",
    "\n",
    "**Suggestion**: try to vary the following options/parameters and analyze its impact:\n",
    "\n",
    "- Experiment a bit more with your preprocessing and see its impact\n",
    "- Try other feature extraction options such as the simple CountVectorizer\n",
    "- Use smaller slices of the dataset to see how the dataset size impacts both baseline and feature selected results\n",
    "- Experiment with other classifiers and the impact of the dimensionality reduction on these\n",
    "- Try to use the model to evaluate other text data, search for republican/democrat posts/news/blogs and see how well your model classifies each, for example political speeches.\n",
    "\n",
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "Although we focused on simple heuristics and in the chi-squared method, there are other features selection methods that you might find useful. Some examples of these are:\n",
    "\n",
    "- Feature selection through variance (sklearn.feature_selection.VarianceThreshold)\n",
    "- Feature selection through mutual information\n",
    "- Recursive feature elimination\n",
    "- Tree-based feature selection\n",
    "\n",
    "**And remember, these methods just tell us that there is a relation between labels and features, but not the nature of that relation.** Now go and apply these methods!\n",
    "\n",
    "![correlation](./media/correlation.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## Annex A.  Details on chi-squared\n",
    "\n",
    "\n",
    "Let's do a really quick example for you to understand how this works. Let's say we are modeling how characteristics from star trek characters that appear in an episode are related to their death, and among our features we have one particular called \"has red t-shirt\" which can take only two categorical values: Yes/No.\n",
    "\n",
    "Let's build a table representing this scenario:\n",
    "\n",
    "|  Has red t-shirt    | Dies   | Does not die | Total |\n",
    "|----------------------|--------|--------------|-------|\n",
    "|         Yes          |   63   |      9       |  72   |\n",
    "|         No           |   13   |     40       |  53   |\n",
    "|         total        |   76   |     49       |  125  |\n",
    "\n",
    "This is what we call a contigency table, and it contains our observed values. Testing for independence of the variables, our expected value is computed from the probabilities $N*P(x_1x_2) = N*P(x_1)P(x_2) $, and we get:\n",
    "\n",
    "|  Has red t-shirt    | Dies   | Does not die | \n",
    "|----------------------|--------|--------------|\n",
    "|         Yes          |  43.78 $$(125 * \\frac{76}{125}\\frac{72}{125})$$ | 28.22 $$(125 * \\frac{49}{125}\\frac{72}{125})$$ |\n",
    "|         No           |  32.22 $$(125 * \\frac{76}{125}\\frac{53}{125})$$ | 20.78 $$(125 * \\frac{49}{125}\\frac{53}{125})$$ |\n",
    "\n",
    "Now we can use these expected values to compute chi-squared using the formula $\\chi^2 = \\sum{\\frac{(O_{x_1x_2} - E_{x_1x_2})^2}{E_{x_1x_2}}}$\n",
    "\n",
    "|  Has red t-shirt    | Dies   | Does not die | \n",
    "|----------------------|--------|--------------|\n",
    "|         Yes          |  8.44 $$(\\frac{(63-43.78)^2}{43.78})$$ | 13.09 $$(\\frac{(9-28.22)^2}{28.22})$$ |\n",
    "|         No           |  11.47 $$(\\frac{(13-32.22)^2}{32.22})$$ | 17.79 $$(\\frac{(40-20.78)^2}{20.78})$$ |\n",
    "\n",
    "Summing these values, we get a chi-squared of 50.79, which corresponds to a p-value < .00001, which means we are dealing with dependent variables. \n",
    "\n",
    "<img src=\"./media/dig3graves.jpeg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-squared in BoW context\n",
    "\n",
    "But how about our context? Our variables are not categorical, so how do we compute this? Actually the chi-squared value can be extended to frequencies, building a contingency table from the feature values and class label. Starting from a table of word frequencies:\n",
    "\n",
    "\n",
    "|                      |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $C_0$ &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   | ... | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $C_j $  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  | ... | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Total               |\n",
    "|----------------------|---------------------|-----|-------------------------|-----|----------------------------|\n",
    "|   Word 0             |  $$C_{t_0, c_0}$$   | ... |  $$C_{t_0, c_1}$$       | ... | $$ \\sum_j{C_{t_0, c_j}} $$ |\n",
    "|   Word 1             |  $$C_{t_1, c_0}$$   | ... |  $$C_{t_1, c_1}$$       | ... | $$ \\sum_j{C_{t_1, c_j}} $$ |\n",
    "|    ...               |   ...               | ... |     ...                 | ... |        ...                 |  \n",
    "|   Word i             |  $$C_{t_i, c_0}$$   | ... |  $$C_{t_i, c_1}$$       | ... | $$ \\sum_j{C_{t_i, c_j}} $$ |\n",
    "|    ...               |   ...               | ... |     ...                 | ... |        ...                 |  \n",
    "|   Total       | $$ \\sum_i{C_{t_i, c_0}} $$ | ... |$$\\sum_i{C_{t_i, c_j}} $$| ... | $$ N = \\sum_{i,j}{C_{t_i, c_j}} $$|  \n",
    "\n",
    "\n",
    "We can take each feature ($t=x_i$) and class ($c=c_j$) and assume a table of the form:\n",
    "\n",
    "|                      | Class j   | Not Class j |  Total | \n",
    "|----------------------|-----------|-------------|--------|\n",
    "|   Word i             |  $C_{tc}$ |   $C_{tx}$  |  $C_{tc}$ + $C_{tx}$ |\n",
    "|   Not Word i         |  $C_{xc}$ |   $C_{xx}$  |  $C_{xc}$ + $C_{xx}$ | \n",
    "|   Total      |  $C_{tc}$ + $C_{xc}$ |   $C_{tx}$ + $C_{xx}$  |  N = $C_{tc}$ + $C_{xc}$ + $C_{tx}$ + $C_{xx}$ |\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C_{tc}$ : counts of co-ocurrences of the term and class \n",
    "- $C_{tx}$ : counts of ocurrences of the term but not the class \n",
    "- $C_{xc}$ : counts of ocurrences of the class but not the term\n",
    "- $C_{xx}$ : counts of ocurrences outside the class and without the term\n",
    "\n",
    "<br>\n",
    "Notice that you can compute your negative word counts (\"Not Word i\") by using the totals:\n",
    "\n",
    "$$C_{xc} = \\sum_i{C_{t_i, c}} - C_{tc}  \\quad\\quad C_{tx} = \\sum_j{C_{t, c_j}} - C_{tc} \\quad\\quad C_{xx} = N - C_{tc} - C_{tx} - C_{xc} $$\n",
    "\n",
    "<br>\n",
    "The expression can be expanded to the following, for each term $t$ and class $c$:\n",
    "\n",
    "$$\\chi^2(t, c) = \\frac{N(C_{tc}C_{xx}-C_{tx}C_{xc})^2}{(C_{tc}+C_{xc})(C_{tx}+C_{xx})(C_{tc}+C_{tx})(C_{xc}+C_{xx})}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "-----\n",
    "\n",
    "**Sugestion**: If, like me, you can't move forward without understanding the origin of these expressions, try to get from the initial expression to this expanded form. However, if you get stuck on the math, just go into the notebook **Learning Notebook - Optional - Chi-squared math** and follow the demonstration.\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "<br>\n",
    "And we will get for all our terms and classes the chi-squared values indication correlation\n",
    "\n",
    "|                      | Republican ($C_0$)  | Democrat ($C_1$) |  \n",
    "|----------------------|-----------|-------------|\n",
    "|   Word 0             |  $$\\chi^2(t_0, c_0)$$ |  $$\\chi^2(t_0, c_1)$$ | \n",
    "|   Word 1             |  $$\\chi^2(t_1, c_0)$$ |  $$\\chi^2(t_1, c_1)$$ | \n",
    "|    ...               |   ...     |     ...     |       \n",
    "|   Word i             |  $$\\chi^2(t_i, c_0)$$ |  $$\\chi^2(t_i, c_1)$$ | \n",
    "|    ...               |   ...     |     ...     | \n",
    "\n",
    "\n",
    "\n",
    "### Chi-squared and TF-IDF\n",
    "\n",
    "You've seen how to apply chi-squared for categorical values and now for frequencies, more specifically for word frequencies. But we were applying it to TF-IDF values, values that seem to violate the chi-squared rules. The reason why we can apply the chi-squared to TF-IDF values is actually because these are just weighted/scaled frequencies and the probabilities and totals should add up.\n",
    "\n",
    "\n",
    "### Implementation, finally!\n",
    "\n",
    "Let's apply this and write a function that receives a matrix with term counts for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(counts):\n",
    "    \"\"\"\n",
    "    Non vectorized version of chi squared function - the idea is that you see the relation with the formula above, \n",
    "    but you should never use such an inefficient version when actually performing a chi-squared analysis\n",
    "    \"\"\"\n",
    "    print(\"Applying chi-squared to {} feature and {} classes\".format(counts.shape[0], counts.shape[1]))\n",
    "    chi_values = np.zeros(counts.shape)\n",
    "    for i in range(counts.shape[0]):\n",
    "        for j in range(counts.shape[1]):\n",
    "            n = counts.sum()\n",
    "            c_tc = counts[i,j]\n",
    "            c_tx = counts.sum(axis=1)[i,0]-c_tc\n",
    "            c_xc = counts.sum(axis=0)[0,j]-c_tc\n",
    "            c_xx = n-c_tc-c_tx-c_xc\n",
    "            chi_values[i,j] = n*(((c_tc*c_xx)-(c_tx*c_xc))**2)/((c_tc+c_xc)*(c_tx+c_xx)*(c_tc+c_tx)*(c_xc+c_xx))\n",
    "    return chi_values\n",
    "\n",
    "def chi_squared_vect(counts):\n",
    "    \"\"\"\n",
    "    Vectorized version of chi squared function - this is still a non-optimized version, but it should run faster than \n",
    "    the previous function\n",
    "    \"\"\"\n",
    "    print(\"Applying chi-squared to {} feature and {} classes\".format(counts.shape[0], counts.shape[1]))\n",
    "    n = counts.sum()\n",
    "    c_tc = counts\n",
    "    c_tx = counts.sum(axis=1)-counts\n",
    "    c_xc = counts.sum(axis=0)-counts\n",
    "    c_xx = n * np.ones(counts.shape) - counts - c_tx - c_xc\n",
    "    num = n * np.square(np.multiply(c_tc, c_xx)-np.multiply(c_tx, c_xc))\n",
    "    den = np.multiply(np.multiply(np.multiply(c_tc+c_xc, c_tx+c_xx), c_tc+c_tx), c_xc+c_xx)\n",
    "    chi_values = np.divide(num, den)\n",
    "    return chi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying to our previous example\n",
    "\n",
    "Now we'll apply our functions to a small portion of our data (first 100 tweets) since our implementation is not optimized, in particular the non-vectorized example. If we find the features with higher chi-values, we find our more important features, which are the ones with less independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying chi-squared to 138409 feature and 2 classes\n",
      "Most important features:\n",
      "\n",
      "chairman, value: 24.846272877542923\n",
      "code, value: 12.933225558706729\n",
      "hearing, value: 14.19673500702519\n",
      "reform, value: 14.104207101906178\n",
      "tax, value: 22.33857645946327\n",
      "tax reform, value: 14.347639988872228\n",
      "taxcutsandjobsact, value: 14.522494937205447\n",
      "taxreform, value: 26.959234731744832\n",
      "texas, value: 15.279411340041444\n",
      "the taxcutsandjobsact, value: 12.734023819258736\n"
     ]
    }
   ],
   "source": [
    "small_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "small_vectorizer.fit(train_data.Tweet)\n",
    "small_X_train = small_vectorizer.transform(train_data.Tweet)\n",
    "small_y_train = train_data.Party\n",
    "\n",
    "idx_rep = np.where(small_y_train=='Republican') \n",
    "idx_dem = np.where(small_y_train=='Democrat') \n",
    "\n",
    "counts_rep = small_X_train[idx_rep[0], :].sum(axis=0)\n",
    "counts_dem = small_X_train[idx_dem[0], :].sum(axis=0)\n",
    "counts = np.concatenate((counts_rep, counts_dem))\n",
    "\n",
    "# chi_values = chi_squared(counts.transpose())\n",
    "chi_values_vect = chi_squared_vect(counts.transpose())\n",
    "\n",
    "feature_names = small_vectorizer.get_feature_names()\n",
    "\n",
    "best_features = chi_values_vect.argsort(axis=0).tolist()\n",
    "\n",
    "print(\"Most important features:\\n\")\n",
    "for idx in sorted(best_features[-10:]):\n",
    "    print(u\"{}, value: {}\".format(feature_names[idx[0]], chi_values_vect[idx[0], 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we got to the same results as the scikit-learn! Time to move on to the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
