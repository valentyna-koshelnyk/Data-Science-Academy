{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/spam_cleaned.tsv', sep='\\t')\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['email'] = df['email'].map(simple_tokenizer)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['email'], df['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9bab744c327dc14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1.\n",
    "\n",
    "The year is 1998 and your hotmail account is starting to fill up with garbage! Luckily you know some NLP and have a handy dataset to train a spam/not spam classifier with. (But beware, the dataset is very imbalanced.) To start, you'll train a baseline classifier using features from TFIDF.\n",
    "\n",
    "\n",
    "### Q1.a)\n",
    "\n",
    "Don't forget your computer is also from 1998 and your computing power is pretty bad, so we'll have to limit this classifier to only using 50 features.\n",
    "Implement a function that returns the fitted vectorizer, and precision and recall, respectively, on the spam class from a K Nearest Neighbors classifier using default TFIDF features (except max_features=50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5802f58ce8de2c27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_spam_tfidf(X_train, y_train, X_test, y_test):\n",
    "    '''Returns a fitted TfidfVectorizer and the test precision and recall on the 'spam' class\n",
    "       from a KNeighborsClassifier trained on the inputted train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer with max_features == 50, fitted to X_train\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the vectorized training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the vectorized training data\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=50)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    clf =  KNeighborsClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    recall = recall_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    precision =  precision_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    \n",
    "    return vectorizer, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-32e077c8cb1aea12",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, precision, recall = get_precision_recall_of_spam_tfidf(X_train, y_train, X_test, y_test)\n",
    "\n",
    "assert math.isclose(precision, 0.8395061728395061)\n",
    "assert math.isclose(recall, 0.6210045662100456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4dd063a2fd408be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.b)\n",
    "\n",
    "Let's see what we can do to get both precision and recall up. We don't want to be bombarded with spam but we also don't want to miss too many real emails.\n",
    "\n",
    "First, look at the selected features of the fitted tfidf vectorizer, sorted by their inverse document frequency (in descending order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4c04022dcbf77d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_ngrams_sorted_by_idf(fitted_vectorizer):\n",
    "    '''Returns the features of a fitted TfidfVectorizer ordered by their idf score\n",
    "    \n",
    "    Parameters:\n",
    "        fitted_vectorizer (TfidfVectorizer): A fitted TfidfVectorizer\n",
    "    \n",
    "    Returns:\n",
    "        ngrams_sorted (list): The features of fitted_vectorizer sorted in ascending order\n",
    "                              by their idf score\n",
    "    '''\n",
    "# YOUR CODE HERE\n",
    "    idf = vectorizer.idf_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_ids = np.argsort(idf)[::-1]\n",
    "    ngrams_sorted = [feature_names[i] for i in topn_ids]\n",
    "\n",
    "\n",
    "    return ngrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-df8fce77a9a2c576",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(hashlib.sha256(get_ngrams_sorted_by_idf(vectorizer)[5].encode()).hexdigest() == \"5ef5ef0364b6939c4ca61f34b393f7b368d1be8619647aaf83d5b395919ab629\")\n",
    "assert(hashlib.sha256(get_ngrams_sorted_by_idf(vectorizer)[48].encode()).hexdigest() == \"bb0347a468d97e98a9c00e37cebec1ab930f6f1221cae0f1fbb92b07e1900ba2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c)\n",
    "\n",
    "Let's also check the number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52cf310a681078ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_number_of_features(fitted_vectorizer):\n",
    "    '''Returns the number of features of a fitted TfidfVectorizer\n",
    "    \n",
    "    Parameters:\n",
    "        fitted_vectorizer (TfidfVectorizer): A fitted TfidfVectorizer\n",
    "    \n",
    "    Returns:\n",
    "        vocab_size (int): The number of features of fitted_vectorizer \n",
    "    '''\n",
    "# YOUR CODE HERE\n",
    "    vocab_size = len(vectorizer.get_feature_names())\n",
    "    return vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6cf4827c6a138b1b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features = get_number_of_features(vectorizer)\n",
    "\n",
    "assert num_features == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31f534c93e127893",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.d)\n",
    "\n",
    "Hmm, looks like we might need to do some feature selection!\n",
    "\n",
    "To try something simple, reimplement the KNN function from 1a, but avoiding English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-124715539ede6b28",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_spam_tfidf_no_stopwords(X_train, y_train, X_test, y_test):\n",
    "    '''Returns a fitted TfidfVectorizer with English stopwords removed and the test precision\n",
    "       and recall on the 'spam' class from a KNeighborsClassifier trained on the inputted train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer with max_features=50 and stopwords='english',\n",
    "                                      fitted to X_train\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the vectorized training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the vectorized training data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    vectorizer = TfidfVectorizer(stop_words ='english', max_features=50)\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)    \n",
    "    clf =  KNeighborsClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    recall = recall_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    precision =  precision_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    \n",
    "    \n",
    "    return vectorizer, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-89cb64b5c51affeb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_stopwords, precision_stopwords, recall_stopwords = \\\n",
    "                    get_precision_recall_of_spam_tfidf_no_stopwords(X_train, y_train, X_test, y_test)\n",
    "\n",
    "assert math.isclose(precision_stopwords, 0.8636363636363636)\n",
    "assert math.isclose(recall_stopwords, 0.6073059360730594)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['then',\n",
       " 'out',\n",
       " 'free',\n",
       " 'know',\n",
       " 'll',\n",
       " 'all',\n",
       " 'gt',\n",
       " 'lt',\n",
       " 'go',\n",
       " 'from',\n",
       " 'when',\n",
       " 'ok',\n",
       " 'how',\n",
       " 'what',\n",
       " 'up',\n",
       " 'this',\n",
       " 'ur',\n",
       " 'no',\n",
       " 'if',\n",
       " 'with']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at most specific features again\n",
    "get_ngrams_sorted_by_idf(vectorizer_stopwords)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b41a6d2d1f3ff2da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2.\n",
    "\n",
    "Not bad, not great, after some simple feature selection we have better precision but slightly worse recall. Still you are confident you can push it a bit further.\n",
    "\n",
    "Repeat Q1, but this time, in addition to stopwords, use the chi-squared method to get the relevant 50 features, and output the fitted vectorizer, fitted SelectKBest, precision, and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35e5bcb64966cd8f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_spam_chi_squared(X_train, y_train, X_test, y_test):\n",
    "    '''Returns a fitted TfidfVectorizer, a fitted SelectKBest and the test precision and recall on\n",
    "       the 'spam' class from a KNeighborsClassifier trained on the inputted train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer with  stopwords='english', fitted to X_train\n",
    "        ch2 (SelectKBest): SelectKBest with score function chi2 and k=50, fitted to vectorized X_train\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the feature-selected training data\n",
    "        recall (float): The recall score of the spam class on the test data from a KNeighborsClassifier\n",
    "                        fitted to the feature-selected training data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words ='english')\n",
    "    vectorizer.fit(X_train)\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    ch2 = SelectKBest(chi2, k=50)\n",
    "    ch2.fit(X_train, y_train)\n",
    "    X_train = ch2.transform(X_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    clf =  KNeighborsClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    precision =  precision_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    recall = recall_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return vectorizer, ch2, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                 min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " SelectKBest(k=50, score_func=<function chi2 at 0x118c147b8>),\n",
       " 0.9101123595505618,\n",
       " 0.7397260273972602)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_precision_recall_of_spam_chi_squared(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9ec326f8b2022b2d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_ch2, ch2, precision_ch2, recall_ch2 = \\\n",
    "                    get_precision_recall_of_spam_chi_squared(X_train, y_train, X_test, y_test)\n",
    "\n",
    "assert math.isclose(precision_ch2, 0.9101123595505618)\n",
    "assert math.isclose(recall_ch2, 0.7397260273972602)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '100', '1000', '10p', '150p', '150ppm', '16', '18', '50', '500', '5000', 'apply', 'award', 'awarded', 'camera', 'cash', 'claim', 'collection', 'com', 'contact', 'cs', 'draw', 'entry', 'free', 'guaranteed', 'landline', 'latest', 'line', 'mobile', 'national', 'nokia', 'po', 'prize', 'rate', 'receive', 'reply', 'ringtone', 'service', 'stop', 'text', 'tone', 'tones', 'txt', 'uk', 'urgent', 'video', 'weekly', 'win', 'won', 'www']\n"
     ]
    }
   ],
   "source": [
    "# Again, we'll check the most important features\n",
    "most_important_features = [vectorizer_ch2.get_feature_names()[i] for i in ch2.get_support(indices=True)]\n",
    "print(most_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14b0595927829765",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3.\n",
    "\n",
    "Chi-squared feature selection helped get both metrics up a little! \n",
    "\n",
    "You now feel confident enough to verify if these features are in fact meaningful for classifying your data, so you decide to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_punct = lambda s: re.sub(r'\\s+\\W*[a-z]?\\W*\\s+', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents that contains the word(s) \"000\"\n",
      "----\n",
      "spam    115\n",
      "ham       3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"100\"\n",
      "----\n",
      "spam    71\n",
      "ham      2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"1000\"\n",
      "----\n",
      "spam    35\n",
      "ham      1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"10p\"\n",
      "----\n",
      "spam    23\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"150p\"\n",
      "----\n",
      "spam    102\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"150ppm\"\n",
      "----\n",
      "spam    35\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"16\"\n",
      "----\n",
      "spam    66\n",
      "ham      2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"18\"\n",
      "----\n",
      "spam    91\n",
      "ham      1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"50\"\n",
      "----\n",
      "spam    255\n",
      "ham       4\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"500\"\n",
      "----\n",
      "spam    65\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"5000\"\n",
      "----\n",
      "spam    31\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"apply\"\n",
      "----\n",
      "spam    25\n",
      "ham      2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"award\"\n",
      "----\n",
      "spam    41\n",
      "ham      1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"awarded\"\n",
      "----\n",
      "spam    26\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"camera\"\n",
      "----\n",
      "spam    25\n",
      "ham      1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"cash\"\n",
      "----\n",
      "spam    51\n",
      "ham      8\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"claim\"\n",
      "----\n",
      "spam    85\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"collection\"\n",
      "----\n",
      "spam    19\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"com\"\n",
      "----\n",
      "ham     276\n",
      "spam     85\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"contact\"\n",
      "----\n",
      "spam    39\n",
      "ham     10\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"cs\"\n",
      "----\n",
      "spam    63\n",
      "ham     19\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"draw\"\n",
      "----\n",
      "spam    28\n",
      "ham      4\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"entry\"\n",
      "----\n",
      "spam    21\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"free\"\n",
      "----\n",
      "spam    146\n",
      "ham      47\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"guaranteed\"\n",
      "----\n",
      "spam    36\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"landline\"\n",
      "----\n",
      "spam    25\n",
      "ham      1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"latest\"\n",
      "----\n",
      "spam    28\n",
      "ham      3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"line\"\n",
      "----\n",
      "spam    66\n",
      "ham     30\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"mobile\"\n",
      "----\n",
      "spam    107\n",
      "ham      12\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"national\"\n",
      "----\n",
      "spam    17\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"nokia\"\n",
      "----\n",
      "spam    40\n",
      "ham      3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"po\"\n",
      "----\n",
      "ham     184\n",
      "spam    147\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"prize\"\n",
      "----\n",
      "spam    61\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"rate\"\n",
      "----\n",
      "spam    31\n",
      "ham     21\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"receive\"\n",
      "----\n",
      "spam    25\n",
      "ham      5\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"reply\"\n",
      "----\n",
      "spam    67\n",
      "ham     35\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"ringtone\"\n",
      "----\n",
      "spam    31\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"service\"\n",
      "----\n",
      "spam    48\n",
      "ham      2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"stop\"\n",
      "----\n",
      "spam    81\n",
      "ham     33\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"text\"\n",
      "----\n",
      "spam    99\n",
      "ham     70\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"tone\"\n",
      "----\n",
      "spam    58\n",
      "ham      3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"tones\"\n",
      "----\n",
      "spam    22\n",
      "ham      1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"txt\"\n",
      "----\n",
      "spam    131\n",
      "ham      13\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"uk\"\n",
      "----\n",
      "spam    59\n",
      "ham      3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"urgent\"\n",
      "----\n",
      "spam    43\n",
      "ham      3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"video\"\n",
      "----\n",
      "spam    20\n",
      "ham      1\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"weekly\"\n",
      "----\n",
      "spam    19\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"win\"\n",
      "----\n",
      "spam    71\n",
      "ham     34\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"won\"\n",
      "----\n",
      "ham     63\n",
      "spam    58\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"www\"\n",
      "----\n",
      "spam    72\n",
      "ham      2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in most_important_features:\n",
    "    print('Documents that contains the word(s) \"%s\"' % feature)\n",
    "    print('----')\n",
    "    docs = X_train.apply(sub_punct).str.lower().str.contains(feature)\n",
    "    print(str(y_train[docs].value_counts()) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9173a0b6dfe2e4d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4.\n",
    "\n",
    "You now want dimensionality reduction techniques like SVD and PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for random\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20da2f33c118bba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.a)\n",
    "\n",
    "Transform the training data with tfidf avoiding stopwords, and calculate the total variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee2c9e7a3e39a7b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer = \n",
    "# X_train_vec = \n",
    "# X_test_vec = \n",
    "# total_variance = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "vectorizer = TfidfVectorizer(stop_words ='english')\n",
    "vectorizer.fit(X_train)\n",
    "X_train_vec = vectorizer.transform(X_train).toarray()\n",
    "X_test_vec =  vectorizer.transform(X_test).toarray()\n",
    "total_variance = np.var(X_train_vec, axis = 0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-75e25a3bbae50fd6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(total_variance, 0.9897797614643888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2188266694d43ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.b)\n",
    "\n",
    "Write a function that fits sklearn's TruncatedSVD with 50 components to the tfidf data, trains a KNN classifier, and outputs the total explained variance, precision, and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43a51c434987d044",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_svd(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    '''Returns the total explained variance of SVD fitted to the Tfidf-vectorized input data,\n",
    "       and the test precision and recall on the 'spam' class from a KNeighborsClassifier\n",
    "       trained on the dimensionality-reduced train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (Series): Text data for training vectorized by Tfidf\n",
    "        y_train (Series): Labels corresponding to X_train_vec\n",
    "        X_test_vec (Series): Text data for testing vectorized by Tfidf\n",
    "        y_test (Series): Labels corresponding to X_test_vec\n",
    "\n",
    "    Returns:\n",
    "        explained variance (float): The total explained variance of a fitted TruncatedSVD\n",
    "                                    with n_components=50 and random_state=seed\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the SVD'd training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the SVD'd training data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    " \n",
    "    svd = TruncatedSVD(n_components=50, random_state=seed)\n",
    "    svd.fit(X_train_vec)\n",
    "\n",
    "\n",
    "    \n",
    "    X_train_vec= svd.transform(X_train_vec)\n",
    "    X_test_vec =  svd.transform(X_test_vec)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf =  KNeighborsClassifier()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "    explained_variance = svd.explained_variance_.sum()\n",
    "    precision =  precision_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    recall = recall_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "\n",
    "\n",
    "\n",
    "    return explained_variance, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16416346460759232, 0.9109947643979057, 0.7945205479452054)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_precision_recall_of_svd(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1776b66bd0ef9237",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "var, precision_svd, recall_svd = \\\n",
    "                    get_precision_recall_of_svd(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "assert math.isclose(var, 0.16416346460759212)\n",
    "assert math.isclose(precision_svd, 0.9109947643979057)\n",
    "assert math.isclose(recall_svd, 0.7945205479452054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4388cbd2be8ec8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5\n",
    "\n",
    "SVD got us a few points higher in recall than chi-squared, nice! Remember now some of the pros and cons of SVD and other dimensionality reduction techniques. While SVD performed better, likely because in theory it can capture information from more than 50 ngrams (whereas in chi-squared the number of features was in a direct correspondence with the number of ngrams), with SVD we lose interpretability and can't examine the most important features!\n",
    "\n",
    "Now let's try PCA.\n",
    "\n",
    "Write a function that fits sklearn's PCA with 50 components to the tfidf data from Q4, trains a KNN classifier, and outputs the total explained variance, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c323629aaea537dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall_of_pca(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    '''Returns the total explained variance of PCA fitted to the Tfidf-vectorized input data,\n",
    "       and the test precision and recall on the 'spam' class from a KNeighborsClassifier\n",
    "       trained on the dimensionality-reduced train data\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (Series): Text data for training vectorized by Tfidf\n",
    "        y_train (Series): Labels corresponding to X_train_vec\n",
    "        X_test_vec (Series): Text data for testing vectorized by Tfidf\n",
    "        y_test (Series): Labels corresponding to X_test_vec\n",
    "\n",
    "    Returns:\n",
    "        explained variance (float): The total explained variance of a fitted PCA\n",
    "                                    with n_components=50 and random_state=seed\n",
    "        precision (float): The precision score of the spam class on the test data from a\n",
    "                           KNeighborsClassifier fitted to the PCA'd training data\n",
    "        recall (float): The recall score of the spam class on the test data from a\n",
    "                        KNeighborsClassifier fitted to the PCA'd training data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    pca = PCA(n_components=50, random_state=seed)\n",
    "    pca.fit(X_train_vec)\n",
    "\n",
    "\n",
    "    \n",
    "    X_train_vec= pca.transform(X_train_vec)\n",
    "    X_test_vec =  pca.transform(X_test_vec)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf =  KNeighborsClassifier()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "    explained_variance = pca.explained_variance_.sum()\n",
    "    precision =  precision_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "    recall = recall_score(y_test, y_pred,  pos_label=\"spam\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return explained_variance, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f0560b07cb8e8132",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "var_pca, precision_pca, recall_pca = \\\n",
    "                    get_precision_recall_of_pca(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "assert math.isclose(var_pca, 0.1658261296302505)\n",
    "assert math.isclose(precision_pca, 0.9067357512953368)\n",
    "assert math.isclose(recall_pca, 0.7990867579908676)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b17e715e9161796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q6.\n",
    "\n",
    "Now we'll change gears a bit and look into word vectors. In Learning Notebook 3 we told you that word vectors can be visualized after being projected into 2D space, and we showed you this diagram:\n",
    "\n",
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">\n",
    "\n",
    "Now we'll try to combine what you've learned about word embeddings and PCA to make our own visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.a)\n",
    "\n",
    "First, to get comfortable with spacy, get the vector for the word \"tree\" and return the sum of its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34ae448736fdde6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sum_tree = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "sum_tree = nlp('tree').vector.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cdd4b59c690522a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(sum_tree, 2.3972085, abs_tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.b) \n",
    "\n",
    "Next, write a function that does a fit_transform of sklearn's PCA to a given set of word vectors, using the correct number of components. The function should return the reduced dimension word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c4ffc6d44920880",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_word_vecs(vectors):\n",
    "    '''Returns PCA-reduced word vectors of the input vectors\n",
    "    \n",
    "    Parameters:\n",
    "        vectors (np.array): Word vectors to be reduced\n",
    "\n",
    "    Returns:\n",
    "        reduced_vecs (np.array): Word vectors reduced to the number of dimensions\n",
    "                                 suitable for plotting with random_state=seed\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    pca = PCA(n_components=2, random_state=seed)\n",
    "    pca.fit(vectors)\n",
    "    reduced_vecs = pca.transform(vectors)\n",
    "    \n",
    "    \n",
    "    return reduced_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors = np.array([[0.1, 0.2, 0.3, 0.4], [0.3, 0.5, 0.1, 0.7], [0.8, 0.6, 0.2, 0.4]])\n",
    "reduced_vecs = reduce_word_vecs(test_vectors)\n",
    "\n",
    "reduced_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05bdaa52733db9e2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_vectors = np.array([[0.1, 0.2, 0.3, 0.4], [0.3, 0.5, 0.1, 0.7], [0.8, 0.6, 0.2, 0.4]])\n",
    "reduced_vecs = reduce_word_vecs(test_vectors)\n",
    "\n",
    "assert reduced_vecs.shape == (3,2)\n",
    "assert math.isclose(reduced_vecs[1][1], 0.24736592153367926)\n",
    "assert math.isclose(reduced_vecs[2][0], 0.43388622222454437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create an array of ~100,000 of spacy's word vectors and use your PCA function to reduce them. If you're curious about using the full amount of word vectors, just remove the `if` statement in the below function, but beware it will use a lot of memory!\n",
    "\n",
    "We'll also set a list of words that we'll plot later and force our set of vectors to include these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_plot = ['banana', 'pineapple', 'mango', 'red', 'blue', 'yellow', 'woman', 'man', 'child', 'playing',\n",
    "                 'reading', 'studying', 'nintendo', 'sony', 'xbox', 'sad', 'angry', 'bored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution: if you run this cell more than once, you may get different words than the ones\n",
    "# expected for the rest of the exercises. If that happens, restart the kernel and try again\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "full_vocab_vecs = []\n",
    "vocab_strings = []\n",
    "for tok in list(nlp.vocab.strings):\n",
    "    if tok in words_to_plot or random.random() < 0.07:\n",
    "        full_vocab_vecs.append(nlp.vocab.get_vector(tok))\n",
    "        vocab_strings.append(tok)\n",
    "\n",
    "vocab_array = np.array(full_vocab_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word vectors shape pre-PCA: {}'.format(vocab_array.shape))\n",
    "\n",
    "full_vocab_reduced = reduce_word_vecs(vocab_array)\n",
    "\n",
    "print('Word vectors shape after PCA: {}'.format(full_vocab_reduced.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-93396749fc10fb39",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(full_vocab_reduced[100][0], -3.0945618, abs_tol=0.00001)\n",
    "assert math.isclose(full_vocab_reduced[9999][0], -0.94902855, abs_tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0feaee69a29da92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.c)\n",
    "\n",
    "Time to plot! For this, we'll limit the visualized words to a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for word in words_to_plot:\n",
    "    idx = vocab_strings.index(word)\n",
    "    coords.append(full_vocab_reduced[idx])\n",
    "\n",
    "coords_array = np.array(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.xlim(min([x for x in coords_array[:,0]]), max([x for x in coords_array[:,0]]))\n",
    "plt.ylim(min([y for y in coords_array[:,1]]), max([y for y in coords_array[:,1]]))\n",
    "plt.scatter(coords_array[:,0], coords_array[:,1])\n",
    "\n",
    "for item, x, y in zip(words_to_plot, coords_array[:,0], coords_array[:,1]):\n",
    "    plt.annotate(item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                 ha='right', va='bottom', color='purple', fontsize=14 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dcc429eed65a7f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The PCA seems to have worked! In the diagram we can see similar types of words closer together. But of course, take these visualizations with a grain of salt because it is practically impossible to preserve all distances in a high dimensional space in just 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a980c450673e4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6.d) \n",
    "\n",
    "As a final exercise, we'll look at some word similarities.\n",
    "\n",
    "Write a function that returns the next closest word in terms of cosine similarity to a given word. If there are multiple words with the same highest similarity, return all of them.\n",
    "\n",
    "Hint: you can use the already-imported `cosine_similarity` function from sklearn to compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72bc3a66c2a8f550",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def closest_word(input_word, words_in_vocab, word_vectors):\n",
    "    '''Returns a list of the closest word or words to the input word, based on cosine similarities\n",
    "       of the word vectors given\n",
    "    \n",
    "    Parameters:\n",
    "        input_word (string): Search for the closest words to this word\n",
    "        words_in_vocab (list): Vocabulary associated with the vectors in word_vectors\n",
    "        word_vectors (np.array): Word vectors associated with the strings in words_in_vocab\n",
    "\n",
    "    Returns:\n",
    "        closest_words_list (list): List of strings containing the closest word or words to\n",
    "                                   input_word, based on cosine similarities of the word_vectors\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return closest_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d06a90ba7d38c797",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'closest_words_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6d4abe9405e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nintendo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_to_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f711da60664c04c146d7a47b722c38a8d0bf46c3f52c2084c5c8d1cb78138e73\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'playing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_to_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"435c149cbc6a5e5cc373cd33347d4c336a22160e06b7df61092b66e56f4d55ec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pineapple'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_to_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"6815f3c300383519de8e437497e2c3e97852fe8d717a5419d5aafb00cb43c494\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-20cb5f518b2b>\u001b[0m in \u001b[0;36mclosest_word\u001b[0;34m(input_word, words_in_vocab, word_vectors)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclosest_words_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'closest_words_list' is not defined"
     ]
    }
   ],
   "source": [
    "assert(hashlib.sha256(closest_word('nintendo', words_to_plot, coords_array)[0].encode()).hexdigest() == \"f711da60664c04c146d7a47b722c38a8d0bf46c3f52c2084c5c8d1cb78138e73\")\n",
    "assert(hashlib.sha256(closest_word('playing', words_to_plot, coords_array)[0].encode()).hexdigest() == \"435c149cbc6a5e5cc373cd33347d4c336a22160e06b7df61092b66e56f4d55ec\")\n",
    "assert(hashlib.sha256(closest_word('pineapple', words_to_plot, coords_array)[0].encode()).hexdigest() == \"6815f3c300383519de8e437497e2c3e97852fe8d717a5419d5aafb00cb43c494\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
