{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Evaluation Metrics for top-*N* Lists\n",
    "\n",
    "When ratings are not available, i.e., with unary data, measuring the rating prediction accuracy isn't possible.\n",
    "\n",
    "In these cases, evaluation is done using $R_{train}$ to learn $L_u$ and evaluating on $R_{test}$\n",
    "\n",
    "Let $T_u \\subset I_u \\cap I_{test}$ the subset of test items that the user $u$ found relevant, e.g., rated positively, clicked, purchased.\n",
    "\n",
    "## 1.1 Precision\n",
    "\n",
    "Precision measures how many recommended items are relevant, out of all recommended items to the user $u$.\n",
    "\n",
    "$$Precision(L_u) = \\frac{|L_u \\cap T_u |}{|L_u|}$$\n",
    "\n",
    "To evaluate the RS as a whole, we average the precision for all active users $u \\in U$.\n",
    "\n",
    "$$Precision(L) = \\frac{\\sum\\limits_{u \\in U} Precision(L_u)}{|U|}$$\n",
    "\n",
    "## 1.2 Recall\n",
    "\n",
    "Recall, on the other side, relates to how many relevant were recommended, out of all relevant items for the user $u$.\n",
    "\n",
    "$$Recall(L_u) = \\frac{|L_u \\cap T_u |}{|T_u|}$$\n",
    "\n",
    "Again, to evaluate the TS we average the results of all active users $u \\in U$.\n",
    "\n",
    "$$Recall(L) = \\frac{\\sum\\limits_{u \\in U} Recall(L_u)}{|U|}$$\n",
    "\n",
    "## 1.3 Average Precision (AP)\n",
    "\n",
    "Precision and recall ignore the ordering. Therefore we need a ranking metric.\n",
    "\n",
    "To understand average precision, we must start with Precision@k and Recall@k, i.e., precision and recall up to cut-off $k$.\n",
    "\n",
    "In other words, we consider only the subset of recommendations $L_u^k \\subset L_u$ from rank 1 through rank $k \\leqslant N$.\n",
    "\n",
    "$$PrecisionAtk(L_u) = \\frac{|L_u^k \\cap T_u |}{|L_u^k|}$$\n",
    "\n",
    "$$RecallAtk(L_u) = \\frac{|L_u^k \\cap T_u |}{|T_u|}$$\n",
    "\n",
    "The AP is a ranking metric, measuring the frequency of relevant recommendations.\n",
    "\n",
    "$$APatN(L_u) = \\frac{\\sum\\limits_{k = 1}^N (PrecisionAtk(L_u) \\cdot relevant(k^{th})}{|T_u|}$$\n",
    "\n",
    "The $relevant(k^{th})$ bit is a boolean value, indicating whether the $k$-th element is relevant, or not.\n",
    "\n",
    "Every hit is valued as how many correct recommendations $|L_u^k \\cap T_u|$ we have up to the rank $k$, out of all recommendations $|L_u^k|$.\n",
    "\n",
    "A first interpretation is that the AP increases only with correct recommendations (what a surprise!).\n",
    "\n",
    "Also, early hits, i.e., front-loading correct recommendations, carry over and are continuously rewarded.\n",
    "\n",
    "Finally, the AP can never decrease as you increase $N$.\n",
    "\n",
    "There is, however, an alternative formula for AP, in terms of both precision and the change in recall from the subset $k$ âˆ’ 1 to the $k$-th.\n",
    "\n",
    "$$APatN(L_u) = \\sum\\limits_{k=1}^NPrecisionAtk(L_u) * \\Delta RecallAtk(L_u)$$ \n",
    "\n",
    "## 1.4 Mean Average Precision (mAP)\n",
    "\n",
    "The Average Precision (AP) is further averaged over all users and reported as a single score.\n",
    "\n",
    "$$mAPatN(L) = \\frac{\\sum\\limits_{u \\in U} APatN(L_u)}{|U|}$$\n",
    "\n",
    "This way, we use a metric that considers both the number and the ranking of hits, i.e., useful recommendations.\n",
    "\n",
    "In this last section, we learned how to use unary data, make predictions based on it and how to evaluate our algorithms.\n",
    "\n",
    "Time to practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
